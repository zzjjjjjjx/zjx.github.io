[{"content":"springcloud eureka eureka的作用\n消费者该如何获取服务提供者具体信息？\n服务提供者启动时向eureka注册自己的信息,eureka保存这些信息消费者,根据服务名称向eureka拉取提供者信息\n如果有多个服务提供者，消费者该如何选择？\n服务消费者利用负载均衡算法，从服务列表中挑选一个消费者如何感知服务提供者健康状态?\neureka配置\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 1 2 3 4 5 6 server: port: 10086 eureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka/ Ribbon负载均衡原理 IRule是指定负载均衡的策略\n通过定义IRule实现可以修改负载均衡规则，有两种方式：\n1．代码方式：在order-service中的OrderApplication类中，定义一个新的lRule:\n1 2 3 @BeanpublicIRulee randomRule(){ return newRandomRule(); } 2.配置文件方式:在order-service的application.yml文件中\n添加新的配置也可以修改规则：\n1 2 3 4 userservice: ribbon: NFLoadBalancerRuleClassName：com.netflix.loadbalancer.RandomRule # 负载均衡规则 NFLoadBalancerRuleClassName:com.alibaba.cloud.nacos.ribbon.NacosRule # nacos的负载均衡规则 饥饿加载 Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。\n而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载：\n1 2 3 4 5 ribbon: eager-load: enabled：true #开启饥饿加载 clients： - userservice # 指定对userservice这个服务饥饿加载 Nacos Github地址 https://github.com/alibaba/nacos\nwindow 进入bin目录后 启动命令 startup.cmd -m standalone\n父工程进入nacok\n1 2 3 4 5 6 7 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.5.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 客户端依赖\n1 2 3 4 5 6 \u0026lt;!-- nacos客户端依赖包--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 1 2 3 4 5 6 7 8 spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: 集群名称 namespace: 环境的id ephemeral: false 设置为非临时实例 根据权重负载均衡 Nacos控制台可以设置实例的权重值，0~1之间同集群内的多个实例，权重越高被访问的频率越高权重,设置为0则完全不会被访问\n环境隔离 Nacos中服务存储和数据存储的最外层都是一个名为namespace的东西，用来做最外层隔离\n每个namespace下的服务不可见\n统一配置管理 1 2 3 4 5 \u0026lt;!-- nacos配置管理客户端依赖包--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在userservice中的resource目录添加一个bootstrap.yml文件,这个文件是引导文件,优先级高于application.yml\n1 2 3 4 5 6 7 8 9 10 spring: application: name：userservice #服务名称 profiles: active：dev #开发环境，这里是dev cloud: nacos: server-addr:LocaLhost:8848 #Nacos地址 config: file-extension:yam #文件后缀名 配置完之后项目启动后会去找id是 name+active+file-extension\n配置热更新 在类上面添加注解@RefreshScope或者@ConfigurationProperties注解\n1 2 3 4 5 6 7 8 9 @Component @ConfigurationProperties(prefix =\u0026#34;pattern\u0026#34;) public class PatternProperties{ private String dateformat; } 多环境共享配置 配置优先级 服务名-Rrofile.yaml\u0026gt;服务名称.yaml\u0026gt;本地配置\nNacos 集群 https://blog.csdn.net/weixin_51265669/article/details/137127998\nNacos与eureka对比 1．Nacos与eureka的共同点\n都支持服务注册和服务拉取 都支持服务提供者心跳方式做健康检测 2.Nacos与Eureka的区别\nNacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式 临时实例心跳不正常会被剔除，非临时实例则不会被剔除 Nacos支持服务列表变更的消息推送模式，服务列表更新更及时 Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式 Feign 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 1 2 3 4 @Feignclient(\u0026#34;userservice\u0026#34;) public interface UserClient { @GetMapping(\u0026#34;/user/{id}\u0026#34;)User findById(@PathVariable(\u0026#34;id\u0026#34;) Long id); } 自定义配置 feign的性能优化 Gateway 1 2 3 4 5 6 7 8 9 \u0026lt;!--网关依赖--\u0026gt;\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--nacos服务发现依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 路由断言工厂 过滤器工厂 如果要对所以路由都生效,则可以把过滤工厂写到default下\n全局过滤器 全局过滤器GlobalFilter全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的。而GlobalFilter的逻辑需要自己写代码实现。\n当过滤器的order值一样时，会按照defaultFilter\u0026gt;路由过滤器\u0026gt;GlobalFilter的顺序执行。\n跨域配置 而GlobalFilter的逻辑需要自己写代码实现。 当过滤器的order值一样时，会按照defaultFilter\u0026gt;路由过滤器\u0026gt;GlobalFilter的顺序执行。\n跨域配置 ","date":"2024-06-03T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/%E5%88%9D%E5%A7%8Bspringcloud/","title":"初始SpringCloud"},{"content":"InonoDB的特点 dml操作遵循ACID模型 ,支持事务\n行级锁,提高高并发访问性能\n支持外键\n架构\n内存结构 buffer pool:\n缓冲池是主存中的一个区域,里面可以缓存磁盘上经常操作的真实数据,再执行增删改查操作时,先操作缓冲池中的数据(若没有增加载并缓存),然后再以一定频率刷新道磁盘,减少磁盘io,加快处理速度\n缓冲池以Page页为单位，底层采用链表数据结构管理Page。\n根据状态，将Page分为三种类型:\nfreepage：空闲page，未被使用。\nclean page：被使用page，数据没有被修改过。\ndirtypage：脏页，被使用page，数据被修改过，也中数据与磁盘的数据产生了不一致。\nChangeBuffer：\n更改缓冲区（针对于非唯一二级索引页），在执行DML语句时，如果这些数据Page没有在BufferPool中，不会直接操作磁盘，而会将数据变更存在更改缓冲区ChangeBuffer中，在未来数据被读取时，再将数据合并恢复到BufferPool中，再将合并后的数据刷新到磁盘中。\nChangeBuffer的意义是什么？\n与聚集索引不同，二级索引通常是非唯一的，并且以相对随机的顺序插入二级索引。同样，删除和更新可能会影响索引树中不相邻的二级索引页，如果每一次都操作磁盘，会造成大量的磁盘IO。有了ChangeBuffer之后，我们可以在缓冲池中进行合并处理，减少磁盘lO。\nAdaptiveHashIndex：\n自适应hash索引\n用于优化对BufferPool数据的查询。InnoDB存储引擎会监控对表上各索引页的查询，如果观察到hash索引可以提升速度，则建立hash索引，称之为自适应hash索引。自适应哈希索引，无需人工干预，是系统根据情况自动完成。\n参数：adaptive_hash_index\nLogBuffer\n日志缓冲区，用来保存要写入到磁盘中的log日志数据（redolog、undolog），默认大小为16MB，日志缓冲区的日志会定期刷新到磁盘中。如果需要更新、插入或删除许多行的事务，增加日志缓冲区的大小可以节省磁盘I/O。\n参数:\ninnodb_log_buffer_size：缓冲区大小\ninnodb_flush_log_at_trx_commit:日志刷新到磁盘时机\n0：每秒将日志写入并刷新到磁盘一次。\n1：日志在每次事务提交时写入并刷新到磁盘\n2：日志在每次事务提交后写入，并每秒刷新到磁盘一次。\n磁盘结构 SystemTablespace：\n系统表空间是更改缓冲区的存储区域。如果表是在系统表空间而不是每个表文件或通用表空间中创建的，它也可能包含表和索引数据。（在MySQL5.x版本中还包含lnnoDB数据字典、undolog等)\n参数：innodb_data_file_path\nFile-Per-TableTablespaces：每个表的文件表空间包含单个InnoDB表的数据和索引l，并存储在文件系统上的单个数据文件中。\n参数：innodb_file_per_table\nGeneralTablespaces：通用表空间，需要通过CREATETABLESPACE语法创建通用表空间，在创建表时，可以指定该表空间。\nUndoTablespaces：撤销表空间，MySQL实例在初始化时会自动创建两个默认的undo表空间（初始大小16M），用于存储undolog日志。\nTemporaryTablespaces：InnoDB使用会话临时表空间和全局临时表空间。存储用户创建的临时表等数据。\nDoublewriteBufferFiles：双写缓冲区，innoDB引擎将数据页从BufferPool刷新到磁盘前，先将数据页写入双写缓冲区文件中，便于系统异常时恢复数据。#ib_16384_0.dblwr#ib_16384_1.dblwrRedoLog：重做日志，是用来实现事务的持久性。该日志文件由两部分组成：重做日志缓冲a（redologbuffer）以及重做日志文件（redolog），前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都会存到该日志中，用于在刷新脏页到磁盘时，发生错误时，进行数据恢复使用。\n后台线程 AIO 异步非阻塞io\nredo log日志 重做日志，记录的是事务提交时数据页的物理修改，是用来实现事务的持久性。\n该日志文件由两部分组成：重做日志缓冲（redolog buffer）以及重做日志文件（redo log file），前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都存到该日志文件中，用于在刷新脏页到磁盘,发生错误时，进行数据恢复使用。\nundo log日志 回滚日志，用于记录数据被修改前的信息，作用包含两个：提供回滚和 MVCC(多版本并发控制）,是用来实现事务的原子性\nundo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。\nUndo log销毁：undolog在事务执行时产生，事务提交时，并不会立即删除undolog，因为这些日志可能还用于MVcC。\nUndo log存储：undo log采用段的方式进行管理和记录，存放在前面介绍的 rollback segment 回滚段中，内部包含1024个undo logsegment.\nMVCC 当前读\n读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。对于我们日常的操作，如：select\u0026hellip;lockin share mode(共享锁)，select ..forupdate、update、insert、delete(排他锁) 都是一种当前读。\n快照读\n简单的select（不加锁）就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。\nRead Committed：每次select，都生成一个快照读。\nRepeatable Read：开启事务后第一个select语句才是快照读的地方。\nSerializable：快照读会退化为当前读。\nMVCC 全称 Multi-Version Concurrency Control\n多版本并发控制。指维护一个数据的多个版本，使得读写操作没有冲突，快照读为MySQL实现3MVCc提供了一个非阻塞读功能。MVCC的具体实现，还需要依赖于数据库记录中的三个隐式字段、undo log日志、readView。\ndb_trx_id 最近修改事务id db_poll_ptr 回滚指针,指向这条记录的上一个版本,用于配合undolog db_row_id 隐藏主键,如果表结构没有指定主键,将会生成该隐藏字段 1 ibd2sdi 文件名 查看表空间文件 mvcc实现原理 undolog回滚日志 在insert、update、delete的时候产生的便于数据回滚的日志。\n当insert的时候，产生的undolog日志只在回滚时需要，在事务提交后，可被立即删除。\n而update、delete的时候，产生的undolog日志不仅在回滚时需要，在快照读时也需要，不会立即被删除\nreadview\nMyISAM\n不支持事务,不支持外键,支持表锁,不支持行锁,访问速度快\n文件:\nxxx.sdi:存储表结构信息\nxxx.MYD:存储数据\nxxx.MYI:存储索引\nMemory引擎的表数据存储在内存中的,由于受到硬件问题,或者断电问题的影响,只能将这些表作为临时表或者缓存使用\n特点:内存存放,hash索引存储\nxxx.sid存储数据 ","date":"2024-05-23T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/mysql-inonodb/","title":"Mysql-InonoDB"},{"content":"索引是什么?\n索引是帮助mysql高效获取数据的数据结构(有序)\n优点:提高数据检索的效率,降低数据库的io成本,通过索引列对数据进行排序,降低排序的成本,降低cpu的消耗\n缺点:索引列也是要占用空间的,索引大大提高了提高了查询效率,同时却也降低了更新表的速度\n索引结构分类:\nB+tree索引\nHash索引 通过hash表实现,只能实现精确匹配查询才有效,不支持范围查询\nR-tree空间索引 空间索引是MYISAM的一种特殊索引类型,主要用于地理空间数据类型,通常使用较少\nFull-text全文索引 是一种通过建立倒排索引,快速匹配文档的方式,类似于LUcene,SOlr,Es\nb树:\nb+树\n为什么InnoDB存储引擎选择使用B+tree索引结构?\n相对于二叉树,层级更少,搜索效率高\n对于b树来说,无论是叶子节点还是非叶子节点,都会保留数据,这样导致一页中存储的键值减少,指针跟着减少,要同意保存大量数据,只能增加树的高度,导致性能降低\n索引结构: 索引结构 描述 B+Tree索引 最常见索引类型,大部分都支持 hash索引 底层用hash表实现的,不支持范围查询 R-tree 空间索引是mylsam引擎的一个特殊索引类型,主要用于地理空间数据类型 full-text 是一种倒排索引,快速匹配文档的方式 为什么选择使用B+tree索引结构？ 相对于二叉树，层级更少，搜索效率高；\n对于B-tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储的键值减少，指针跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低;\n相对Hash索引l，B+tree支持范围匹配及排序操作;\n索引分类: 主键索引:针对表中主键创建的索引,默认自动创建,只能有一个 唯一索引: 避免同一个表中某数据列中的值重复 常规索引: 快速定位特定数据 全文索引: 全文索引查找的是文本中的关键词,而不是比较索引中的值 在InnoDB下根据索引的存储形式,又可以分为以下两种:\n聚集索引 将数据存储与索引放到一块,索引结构的叶子节点保存了行数据 (必须有,而且只能有一个) 选取规则:\n如果存在主键,主键索引就是聚集索引,如果不存在,将使用第一个唯一索引作为聚集索引\n如果表中没有主键或者合适的唯一索引,则InnoDB会自动生成一个rowid作为隐藏的聚集索引\n二级索引 将数据和索引分开存储,索引结构的叶子节点关联的是对应的主键 索引语法 1 2 3 4 5 create (unique|fulltext) index index_name on table_name (index_col_name)创建索引 show index from table_name 查看索引 drop index index_name on table_name; 删除索引 sql性能分析: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 show global status like \u0026#39;Com_______\u0026#39; 7个下划线查看每种操作的次数 show variables like \u0026#39;slow_query_log\u0026#39; 查看慢查询日志是否开启 show variables like \u0026#39;slow_query_log_file\u0026#39;查看慢查询日志的位置 profile详情能够在做sql优化帮助我们了解 select @@have_profiling;查看是否支持 show profiles 查看每一条sql耗时的基本情况 show profile for query query_id 查看指定的query_id的sql语句各个阶段的耗时情况 show profile cpu for query query_id查看cpu的使用情况 直接在sql语句前面加上explain\nid : select查询的序列号,表示查询中执行的select子句或者操作表的顺序(id相同,执行顺序从上到下,id不同值越大越先执行\nselect_type:表示select的类型,常见的取值有simple(简单表,即不使用表连接或者子查询),primary(主查询,即外层的查询),union(union中第二个或者后面的查询语句),subquery(select/where之后包含了子查询),derived包含在from子句中的子查询 type: 表示连接类型,性能由好到查的连接类型为NULL,system,const(主键或者唯一索引),eq_ref,ref(用非唯一性的索引),range,index,all\nkey: 实际使用的索引,如果为NULL,则没有使用索引\nkey_len:表示索引使用的字节数,该值为索引字段最大可能长度,并非实际使用长度,在不损失精确性的前提下,长度越短越好\nrow:mysql认为必须要执行查询的行数,在innodb引擎中是一个估计值,可能并不总是准确的\nfiltered:表示返回结果的行数,占需读取行数的百分比,filtered的值越大越好\nextra:using index condition 查询使用了索引但是需要回表查询数据,using where ,using index 查询使用了索引,但是需要的数据都在索引列中能找到,所以不需要回表查询数据\n索引的使用: 最左前缀法则:\n如果索引了多列(联合索引),要遵守最左前缀法则,最左前缀法则指的是查询从索引的最左列开始,并不跳过索引中的列\n如果跳跃某一列,索引将部分失效(后面的字段索引失效)\n注:mysql8之后优化器优化了最左前缀法则,可以进行跳跃,顺序也可以打乱\n索引列运算:\n不要在索引列进行运算操作,索引将失效\n字符串不加引号:\n字符串类型字段使用时,不加引号,索引将失效\n模糊查询\n如果仅仅是尾部模糊匹配,索引不会失效,如果是头部模糊匹配,索引失效\nor连接的条件:\n用or分割开的条件,如果or前的条件中的列有索引,而后面的列中没有索引,那么涉及的索引都不会被用到\n覆盖索引:\n尽量使用覆盖索引(查询使用了索引,并且需要返回的列,在该索引中已经全部能够找到),减少select *就是减少回表操作\n前缀索引:\n当前字段类型为字符串时,有时候需要索引很长的字符串,这会让索引变得很大,查询时,浪费大量的磁盘io,影响查询效率,\n此时可以只将字符串的一部分前缀,建立索引,这样可以大大节约索引空间,从而提高索引效率\n1 create index 名字 table名字(clomn(n)) 索引下推 现在我们知道，对于联合索引（a, b），在执行 select * from table where a \u0026gt; 1 and b = 2 语句的时候，只有 a 字段能用到索引，那在联合索引的 B+Tree 找到第一个满足条件的主键值（ID 为 2）后，还需要判断其他条件是否满足（看 b 是否等于 2），那是在联合索引里判断？还是回主键索引去判断呢？\n在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 当你的查询语句的执行计划里，出现了 Extra 为 Using index condition，那么说明使用了索引下推的优化。\nSql提示: 如果有满足多个索引的话,mysql会使用列多的那个索引\n1 2 3 4 5 use index()你用哪个索引 ignore index()你不用哪个索引 force index()你必须用这个索引 索引的设计原则 1.针对于数据量较大，且查询比较频繁的表建立索引。\n2.针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引l。\n3.尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高。\n4.如果是字符串类型的字段，字段的长度较长，可以针对于字段的特点，建立前缀索引。\n5.尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。\n6.要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价也就越大，会影响增删改的效率。\n7.如果索引I列不能存储NULL值，请在创建表时使用NOTNULL约束它。当优化器知道每列是否包含NULL值时，它可以更好地确定哪个索引最有效地用于查询。\n","date":"2024-05-19T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/mysql%E7%B4%A2%E5%BC%95/","title":"Mysql索引"},{"content":"@TOC\nredisObject对象机制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 typedef struct redisObject{ // 类型 unsigned type:4; // 编码方式 unsigned encoding:4; // LRU - 24位, 记录最末一次访问时间（相对于lru_clock）; 或者 LFU（最少使用的数据：8位频率，16位访问时间） unsigned lru:LRU_BITS; // LRU_BITS: 24 // 引用计数 int refcount; // 指向底层数据结构实例 void *ptr; } robj; 为什么Redis会设计redisObject对象？\n在redis的命令中，用于对键进行处理的命令占了很大一部分，而对于键所保存的值的类型（键的类型），键能执行的命令又各不相同。如： LPUSH 和 LLEN 只能用于列表键, 而 SADD 和 SRANDMEMBER 只能用于集合键, 等等; 另外一些命令, 比如 DEL、 TTL 和 TYPE, 可以用于任何类型的键；但是要正确实现这些命令, 必须为不同类型的键设置不同的处理方式: 比如说, 删除一个列表键和删除一个字符串键的操作过程就不太一样。\n以上的描述说明, Redis 必须让每个键都带有类型信息, 使得程序可以检查键的类型, 并为它选择合适的处理方式\n为了解决以上问题, Redis 构建了自己的类型系统, 这个系统的主要功能包括:\nredisObject 对象.\n基于 redisObject 对象的类型检查.\n基于 redisObject 对象的显式多态函数.\n对 redisObject 进行分配、共享和销毁的机制\ntype记录了对象所保存的值的类型，它的值可能是以下常量中的一个：\n1 2 3 4 5 6 7 8 /* * 对象类型 */ #define OBJ_STRING 0 // 字符串 #define OBJ_LIST 1 // 列表 #define OBJ_SET 2 // 集合 #define OBJ_ZSET 3 // 有序集 #define OBJ_HASH 4 // 哈希表 encoding记录了对象所保存的值的编码，它的值可能是以下常量中的一个： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /* * 对象编码 */ #define OBJ_ENCODING_RAW 0 /* Raw representation */ #define OBJ_ENCODING_INT 1 /* Encoded as integer */ #define OBJ_ENCODING_HT 2 /* Encoded as hash table */ #define OBJ_ENCODING_ZIPMAP 3 /* 注意：版本2.6后不再使用. */ #define OBJ_ENCODING_LINKEDLIST 4 /* 注意：不再使用了，旧版本2.x中String的底层之一. */ #define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */ #define OBJ_ENCODING_INTSET 6 /* Encoded as intset */ #define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */ #define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */ #define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */ #define OBJ_ENCODING_STREAM 10 /* Encoded as a radix tree of listpacks */ redis执行命令的过程 对象共享 redis一般会把一些常见的值放到一个共享对象中，这样可使程序避免了重复分配的麻烦，也节约了一些CPU时间。\nredis预分配的值对象如下：\n各种命令的返回值，比如成功时返回的OK，错误时返回的ERROR，命令入队事务时返回的QUEUE，等等 包括0 在内，小于REDIS_SHARED_INTEGERS的所有整数（REDIS_SHARED_INTEGERS的默认值是10000） 引用计数以及对象的消毁 redisObject中有refcount属性，是对象的引用计数，显然计数0那么就是可以回收。\n每个redisObject结构都带有一个refcount属性，指示这个对象被引用了多少次； 当新创建一个对象时，它的refcount属性被设置为1； 当对一个对象进行共享时，redis将这个对象的refcount加一； 当使用完一个对象后，或者消除对一个对象的引用之后，程序将对象的refcount减一； 当对象的refcount降至0 时，这个RedisObject结构，以及它引用的数据结构的内存都会被释放。 动态字符串SDS redis中保存的key是字符串,value往往是字符串或者字符串的集合,可见字符串是redis中最常用的一种数据结构\nC语言字符串问题:c语言字符串本质是一个字符数组\n获取字符串长度需要通过运算(O(n)) 非二进制安全(字符数组末尾会有个\u0026rsquo;\\0\u0026rsquo;来标记结束) 不可修改 为此redis构建了一种新的字符串结构,称为简单动态字符串简称:SDS\nlen，记录了字符串长度。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。 alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。 flags，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，后面在说明区别之处。 buf[]，字符数组，用来保存实际数据。不仅可以保存字符串，也可以保存二进制数据。 Redis 一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。\n这 5 种类型的主要区别就在于，它们数据结构中的 len 和 alloc 成员变量的数据类型不同。\n比如 sdshdr8它的定义分别如下：\n1 2 3 4 5 6 7 //__attribute__ ((packed)) ，它的作用是：告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐。 struct __attribute__ ((__packed__)) sdshdr8{ uint8_t len; uint8_t alloc; unsigned char flags; char buf[]; }; SDS扩容原理:\n如果所需的 sds 长度小于 1 MB，那么最后的扩容是按照翻倍扩容来执行的，即 2 倍的newlen 如果所需的 sds 长度超过 1 MB，那么最后的扩容长度应该是 newlen + 1MB。 这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用「未使用空间」，而无须执行内存分配，有效的减少内存分配次数。\n链表 常见的双向链表的实现方式\n1 2 3 4 5 6 7 8 typedef struct listNode { //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void *value; } l redis在listNode结构体基础上又封装了list\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct list { //链表头节点 listNode *head; //链表尾节点 listNode *tail; //节点值复制函数 void *(*dup)(void *ptr); //节点值释放函数 void (*free)(void *ptr); //节点值比较函数 int (*match)(void *ptr, void *key); //链表节点数量 unsigned long len; } list 链表的优缺点: 优点:\nlistNode 链表节点的结构里带有 prev 和 next 指针，获取某个节点的前置节点或后置节点的时间复杂度只需O(1)，而且这两个指针都可以指向 NULL，所以链表是无环链表； list 结构因为提供了表头指针 head 和表尾节点 tail，所以获取链表的表头节点和表尾节点的时间复杂度只需O(1)； list 结构因为提供了链表节点数量 len，所以获取链表中的节点数量的时间复杂度只需O(1)； listNode 链表节使用 void* 指针保存节点值，并且可以通过 list 结构的 dup、free、match 函数指针为节点设置该节点类型特定的函数，因此链表节点可以保存各种不同类型的值； 缺点：\n链表每个节点之间的内存都是不连续的，意味着无法很好利用 CPU 缓存。能很好利用 CPU 缓存的数据结构就是数组，因为数组的内存是连续的，这样就可以充分利用 CPU 缓存来加速访问。 还有一点，保存一个链表节点的值都需要一个链表节点结构头的分配，内存开销较大。 因此，Redis 3.0 的 List 对象在数据量比较少的情况下，会采用「压缩列表」作为底层数据结构的实现，它的优势是节省内存空间，并且是内存紧凑型的数据结构。\n不过，压缩列表存在性能问题（具体什么问题，下面会说），所以 Redis 在 3.2 版本设计了新的数据结构 quicklist，并将 List 对象的底层数据结构改由 quicklist 实现。\n然后在 Redis 5.0 设计了新的数据结构 listpack，沿用了压缩列表紧凑型的内存布局，最终在最新的 Redis 版本，将 Hash 对象和 Zset 对象的底层数据结构实现之一的压缩列表，替换成由 listpack 实现。\n压缩链表 压缩列表是 Redis 为了节约内存而开发的，它是由连续内存块组成的顺序型数据结构，有点类似于数组。\nzlbytes字段的类型是uint32_t, 这个字段中存储的是整个ziplist所占用的内存的字节数\nzltail字段的类型是uint32_t, 它指的是ziplist中最后一个entry的偏移量. 用于快速定位最后一个entry, 以快速完成pop等操作\nzllen字段的类型是uint16_t, 它指的是整个ziplit中entry的数量. 这个值只占2bytes（16位）: 如果ziplist中entry的数目小于65535(2的16次方), 那么该字段中存储的就是实际entry的值. 若等于或超过65535, 那么该字段的值固定为65535, 但实际数量需要一个个entry的去遍历所有entry才能得到.\nzlend是一个终止字节, 其值为全F, 即0xff. ziplist保证任何情况下, 一个entry的首字节都不会是255\n前三这个都采用小端的存储方式\n常见的entry的结构为** **\nprevlen：前一个entry的大小 为了实现从后往前遍历； ​ 当前一个元素长度小于254（255用于zlend）的时候，prevlen长度为1个字节，值即为前一个entry的长度，如果长 度大于等于254的时候，prevlen用5个字节表示，第一字节设置为254，后面4个字节存储一个小端的无符号整 型，表示前一个entry的长度；\nencoding：不同的情况下值不同，用于表示当前entry的类型和长度；\n如果当前节点的数据是整数，则 encoding 会使用 1 字节的空间进行编码，也就是 encoding 长度为 1 字节。通过 encoding 确认了整数类型，就可以确认整数数据的实际大小了，比如如果 encoding 编码确认了数据是 int16 整数，那么 data 的长度就是 int16 的大小。\n如果当前节点的数据是字符串，根据字符串的长度大小，encoding 会使用 1 字节/2字节/5字节的空间进行编码，encoding 编码的前两个 bit 表示数据的类型，后续的其他 bit 标识字符串数据的实际长度，即 data 的长度\nentry-data：真是用于存储entry表示的数据；\n在entry中存储的是int类型时，encoding和entry-data会合并在encoding中表示，此时没有entry-data字段；\n1 2 3 4 5 6 7 8 9 typedef struct zlentry{ unsigned int prevrawlensize; //表示 previous_entry_length字段的长度 unsigned int prevrawlen; //表示 previous_entry_length字段存储的内容 unsigned int lensize; //表示 encoding字段的长度 unsigned int len; //表示数据内容长度 unsigned int headersize;//表示当前元素的首部长度，即previous_entry_length字段长度与encoding字段长度之和 unsigned char encoding; //表示数据类型 unsigned char *p; //p表示当前元素首地址 }zlentry; ziplist的缺点 ziplist也不预留内存空间, 并且在移除结点后, 也是立即缩容, 这代表每次写操作都会进行内存分配操作.\n结点如果扩容, 导致结点占用的内存增长, 并且超过254字节的话, 可能会导致链式反应: 其后一个结点的entry.prevlen需要从一字节扩容至五字节. 最坏情况下, 第一个结点的扩容, 会导致整个ziplist表中的后续所有结点的entry.prevlen字段扩容. 虽然这个内存重分配的操作依然只会发生一次, 但代码中的时间复杂度是o(N)级别, 因为链式扩容只能一步一步的计算.\n因此，压缩列表只会用于保存的节点数量不多的场景，只要节点数量足够小，即使发生连锁更新，也是能接受的。\n字典-Dict 哈希表结构\n1 2 3 4 5 6 7 8 9 10 typedef struct dictht { //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码，用于计算索引值 unsigned long sizemask; //该哈希表已有的节点数量 unsigned long used; } dictht 哈希节点结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 typedef struct dictEntry { //键值对中的键 void *key; //键值对中的值 union { void *val; uint64_t u64; int64_t s64; double d; } v; //指向下一个哈希表节点，形成链表 struct dictEntry *next; } dict ​ dictEntry 结构里键值对中的值是一个「联合体 v」定义的，因此，键值对中的值可以是一个指向实际值的指针，或者是一个无符号的 64 位整数或有符号的 64 位整数或double 类的值。这么做的好处是可以节省内存空间，因为当「值」是整数或浮点数时，就可以将值的数据内嵌在 dictEntry 结构里，无需再用一个指针指向实际的值，从而节省了内存空间。\nRedis采用链式哈希存储哈希冲突\nb452135f44fab77e3c578d91666e.png#pic_center)\n链式哈希局限性也很明显，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O(n)。\n要想解决这一问题，就需要进行 rehash，也就是对哈希表的大小进行扩展。\nrehash 扩容和收缩：当哈希表保存的键值对太多或者太少时，就要通过 rehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤： 1、如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2n 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是根据已使用空间缩小一倍创建一个新的哈希表。\nredis定义一个dict结构体,其中两个哈希表\n1 2 3 4 5 6 typedef struct dict { … //两个Hash表，交替使用，用于rehash操作 dictht ht[2]; … } dict; 2、重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。\n3、所有键值对都迁徙完毕后，释放原哈希表的内存空间。\n触发扩容的条件：\n1、服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。\n2、服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。\n负载因子 = 哈希表已保存节点数量 / 哈希表大小\n渐进式rehash 如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求。因此redis采用的是渐进式rehash\n渐进式 rehash 步骤如下：\n给「哈希表 2」 分配空间； 在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上； 随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。 在渐进式hash过程中哈希表1不再进行增加操作,哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。\n整数集-intSet 整数集合本质上是一块连续内存空间，它的结构定义如下：\n1 2 3 4 5 6 7 8 typedef struct intset { //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[]; } intset; encoding 表示编码方式，的取值有三个：INTSET_ENC_INT16, INTSET_ENC_INT32, INTSET_ENC_INT64\nlength 代表其中存储的整数的个数\ncontents 指向实际存储数值的连续内存区域, 就是一个数组；整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值得大小从小到大有序排序，且数组中不包含任何重复项。（虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值，contents 数组的真正类型取决于 encoding 属性的值）\n内存分布图 整数集合的升级 当在一个int16类型的整数集合中插入一个int32类型的值，整个集合的所有元素都会转换成32类型。 整个过程有三步：\n根据新元素的类型（比如int32），扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变。 最后改变encoding的值，length+1。 升级过程\n那么如果我们删除掉刚加入的int32类型时，会不会做一个降级操作呢？\n不会。主要还是减少开销的权衡。\n跳表 - ZSkipList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 typedef struct zskiplistNode { //Zset 对象的元素值 sds ele; //元素权重值 double score; //后向指针 struct zskiplistNode *backward; //节点的level数组，保存每层上的前向指针和跨度 struct zskiplistLevel { struct zskiplistNode *forward; unsigned long span; } level[]; } zskiplistNode; typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length; int level; } zskiplist; 跳表的结构:\n跳表的查找过程\n如果要查找「元素：abcd，权重：4」的节点，查找的过程是这样的：\n先从头节点的最高层开始，找「元素：abc，权重：3」节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点； 但是该层的下一个节点是空节点（ leve[2]指向的是空节点），于是就会跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[1]; 「元素：abc，权重：3」节点的 leve[1] 的下一个指针指向了「元素：abcde，权重：4」的节点，然后将其和要查找的节点比较。虽然「元素：abcde，权重：4」的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据「大于」要查找的数据，所以会继续跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[0]； 「元素：abc，权重：3」节点的 leve[0] 的下一个指针指向了「元素：abcd，权重：4」的节点，该节点正是要查找的节点，查询结束。 跳表的节点层数设置:\n跳表的相邻两层的节点数量的比例会影响跳表的查询性能。\n那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？\n如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。\nRedis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。\n具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。\n这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。\n虽然我前面讲解跳表的时候，图中的跳表的「头节点」都是 3 层高，但是其实如果层高最大限制是 64，那么在创建跳表「头节点」的时候，就会直接创建 64 层高的头节点。\n如下代码，创建跳表时，头节点的 level 数组有 ZSKIPLIST_MAXLEVEL个元素（层），节点不存储任何 member 和 score 值，level 数组元素的 forward 都指向NULL， span值都为0。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* Create a new skiplist. */ zskiplist *zslCreate(void) { int j; zskiplist *zsl; zsl = zmalloc(sizeof(*zsl)); zsl-\u0026gt;level = 1; zsl-\u0026gt;length = 0; zsl-\u0026gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); for (j = 0; j \u0026lt; ZSKIPLIST_MAXLEVEL; j++) { zsl-\u0026gt;header-\u0026gt;level[j].forward = NULL; zsl-\u0026gt;header-\u0026gt;level[j].span = 0; } zsl-\u0026gt;header-\u0026gt;backward = NULL; zsl-\u0026gt;tail = NULL; return zsl; } 其中，ZSKIPLIST_MAXLEVEL 定义的是最高的层数，Redis 7.0 定义为 32，Redis 5.0 定义为 64，Redis 3.0 定义为 32。\n快表-quicklist 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 typedef struct quicklist { //quicklist的链表头 quicklistNode *head; //quicklist的链表头 //quicklist的链表尾 quicklistNode *tail; //所有压缩列表中的总元素个数 unsigned long count; //quicklistNodes的个数 unsigned long len; ... } quicklist; typedef struct quicklistNode { //前一个quicklistNode struct quicklistNode *prev; //前一个quicklistNode //下一个quicklistNode struct quicklistNode *next; //后一个quicklistNode //quicklistNode指向的压缩列表 unsigned char *zl; //压缩列表的的字节大小 unsigned int sz; //压缩列表的元素个数 unsigned int count : 16; //ziplist中的元素个数 .... } quicklistNode; quicklist内存布局图:\n在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。\nquicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题\nlistpack encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码； data，实际存放的数据； len，encoding+data的总长度； listpack如何解决反向遍历的?\n首先访问 listpack 的前4字节得到总长度，然后就可以定位到末尾结尾符位置。然后指针左移就可以访问到最后一个元素的长度 len，指针再左移 len 就可以访问最后一个元素的 encoding，根据编码方式访问元素。指针再左移又可以访问到倒数第2个元素的长度，以此类推。 访问元素长度len字段时，有一个关键点，就是如何判断 len 部分结束了。因为 len 可能占用1字节，也可能占用多个字节。listpack 的做法是，每个字节只使用 7 Bit，最高位来表示是否还要继续读。\n","date":"2024-04-06T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"Redis-底层数据结构"},{"content":"@TOC\nRDB 全程redis database backip file(Redis数据备份文件),也被叫做redis数据快照,简单来说就是把内存中所有数据都记录到磁盘中,\n当redis实例故障重启后,从磁盘读取快照文件,恢复数据.\n快照文件称为RDB文件,默认是保存在当前运行目录\nsave命令 有redis的主进程来执行RDB,会阻塞所有命令 bgsave开启子进程执行RDB,避免主进程受到影响 RDB的配置:\nredis内部有触发RDB的机制,可以在redis.conf文件中找到,格式如下:\nsave 900 1 如果900秒内,如果至少一个key被修改,则执行bgsave,如果是save \u0026quot;\u0026quot; 则表示禁用redis\nRDB的其它配置也可以在redis.conf文件中设置:\nrdbcompression yes 是否压缩,建议不开启,压缩也会消耗cpu,磁盘的话不值钱\ndefilename dump.rdb rdb文件名称\ndir ./ 文件保存的路径目录 AOF AOF全称为APPend Only file(追加文件).redis处理的每一个写命令都会记录在AOF文件,可以看做是命令日志文件\nAOF配置\nAOF默认是关闭的,需要修改redis.conf配置文件来开启AOF\nappendonly yes\nappendfilename \u0026ldquo;appendonly.aof\u0026rdquo;\nAOF的命令记录的频率也是可以通过redis.conf文件来配\nappendfsync always 表示每执行一次写命令,立刻记录到AOF文件\nappednfsync everysec 写命令执行完先放入到AOF缓存区,然后表示每隔1m将缓冲区数据写到AOF文件,是默认方案\nappednfsync no 写命令执行完先放入AOF缓存区,由操作系统决定何时将缓冲区内容写回磁盘\n因为是记录命令,AOF文件会比RDB文件大的多,而且AOF会记录对同一个key的多次写操作,但只有最后一次写操作才有意义,\n通过执行bgrewriteaof命令,可以让AOF执行重写功能,用最少的命令达到相同的效果\n不知道大家注意到没有，Redis 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n第一个好处，避免额外的检查开销。\n因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。\n而如果先执行写操作命令再记录日志的话，只有在该命令执行成功后，才将命令记录到 AOF 日志里，这样就不用额外的检查开销，保证记录在 AOF 日志里的命令都是可执行并且正确的。\n第二个好处，不会阻塞当前写操作命令的执行，因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。\n当然，AOF 持久化功能也不是没有潜在风险。\n第一个风险，执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。\n第二个风险，前面说道，由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前写操作命令的执行，但是可能会给「下一个」命令带来阻塞风险。\nAOF重写机制 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。\n所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。\nAOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。\n重写 AOF 的时候，不直接复用现有的 AOF 文件，而是先写到新的 AOF 文件再覆盖过去。\n因为如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染，可能无法用于恢复使用。\n所以 AOF 重写过程，先重写到新的 AOF 文件，重写失败的话，就直接删除这个文件就好，不会对现有的 AOF 文件造成影响。\nRedis 的重写 AOF 过程是由后台子进程 bgrewriteaof\\来完成的，这么做可以达到两个好处：\n子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本（数据副本怎么产生的后面会说），这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 在重写工程中有两个地方会造成父进程阻塞\n创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；\n创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长；\nAOF和RDB对比 ","date":"2024-03-31T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/redis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96/","title":"Redis的持久化"},{"content":"@TOC\nsynchronized关键字可以实现什么类型的锁？ 悲观锁：synchronized关键字实现的是悲观锁，每次访问共享资源时都会上锁。 非公平锁：synchronized关键字实现的是非公平锁，即线程获取锁的顺序并不一定是按照线程阻塞的顺序。 可重入锁：synchronized关键字实现的是可重入锁，即已经获取锁的线程可以再次获取锁。 独占锁或者排他锁：synchronized关键字实现的是独占锁，即该锁只能被一个线程所持有，其他线程均被阻塞。 轻量级锁 1.轻量级锁:如果一个对象虽然有多线程访问,但多线程访问的时间是错开的(也就是没有竞争),那么可以用轻量级锁来优化\n创建锁记录对象,每个线程的栈帧都会包含一个所记录的结构,内存可以存储锁定对象的mark word 让锁记录的object reference指向锁对象,并尝试用cas替换object的mark word,将mark word的值存入锁记录\n如果cas替换成功,对象中存储了所记录地址和状态00,表示由该线程给对象加锁\n如果cas失败,有两种情况:\n如果是其他线程持有了该object的轻量级锁,这时表明有竞争,进入锁膨胀过程 如果是自己执行了synchronized锁重入,那么再添加一条lock record作为重入的计数 当退出代码块的时候(解锁时)如果有取值为null的锁记录,表示有重入,这时重置锁记录,表示重入计数减一 当退出代码块的时候(解锁时)取值不为null,这时使用cas把mark word的值恢复给对象头,成功则解锁成功,失败说明轻量级锁进行了锁膨胀或已经升级为重量级锁,进入重量级锁解锁过程 如果尝试加轻量级锁的过程,cas操作无法成功,这是一种情况就是其他线程对此对象加上了轻量级锁(有竞争),这时需要锁膨胀,将轻量级锁变成重量级锁\n当退出时会cas失败,然后进入到monitor中将owner设置为null,韩星entrylist的blocked线程\n偏向锁 一个对象创建时默认开启偏向锁\n撤销:调用了对象的hashcode,但偏向锁的对象markword中存储是线程id,如果掉桶hashcode会导致偏向锁被撤销\n轻量级锁会在锁记录中记录hashcode 重量级会在monitor中存储 批量重偏向 如果对象虽然被多个线程访问,但没有竞争,这是偏向了t1的对想仍有机会重新偏向t2,重偏向会重新设置对象的thread id\n当撤销偏向锁阙值超过20次,jvm就会这样觉得,我是不是偏向错了呢,于是会在给这些对象加锁时重新偏向至加锁线程\n批量撤销 当撤销偏向到达四十次,于是整个类的所有对象都会变成不可偏向的,新建的对象也是不可偏向的\n锁撤销 当编译器认为没有多线程竞争的时候就会把锁消除进行优化 ,重偏向会重新设置对象的thread id\n当撤销偏向锁阙值超过20次,jvm就会这样觉得,我是不是偏向错了呢,于是会在给这些对象加锁时重新偏向至加锁线程\n批量撤销 当撤销偏向到达四十次,于是整个类的所有对象都会变成不可偏向的,新建的对象也是不可偏向的\n锁消除 当编译器认为没有多线程竞争的时候就会把锁消除进行优化\n","date":"2024-03-17T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/synchronized%E5%8E%9F%E7%90%86/","title":"synchronized原理"},{"content":"@TOC\n垃圾收集的算法从判定对象消亡的的角度除法,可以分为引用计数式垃圾收集和追踪式垃圾收集两大类,这两大类常被称为直接垃圾收集和间接垃圾收集\n分代收集理论: 弱分代假说:绝大多数对象都是朝生夕死的 强分代假说:熬过越多次垃圾收集过程的对象就越难以消亡 把那些朝生夕死的放到一个区域,就能以极低的代价回收到大量的空间,把难以消亡的对象集中到一起,虚拟机就可以以较低的 频率去回收这个区域,兼顾了垃圾回收的时间开销和内存的空间有效利用\n**存在问题:**对象不是相互独立的,对象之间是可以相互引用的,假如要进行一次局限于新生代的收集,但新生代中的对象完全有可能被老年代所引用的,为了找出区域中的存活对象,我们不能不在固定的GC Roots中加入整个老年代中所有对象来确保可达性分析结果的正确性,这无疑给内存回收带来了很大的负担,为了解决这个问题,在分代收集理论的基础上加入了第三条法则\u0026quot;跨代引用相对同代引用来说占极少数\u0026quot;\n解决方案:根据第三条假说,我们就不应再为少量的跨代引用去扫描整个老年代,也不必浪费空间专门记录每个对象是否存在及存在哪些跨年代引用,只需在新生代上建立一个全局的数据结构(记忆集),这个结构把老年代划分为若干个小块,标识出那块内存会存在跨代引用,之后发生minor GC的时候只需要把这些对象加入到GC Roots中进行扫描\n垃圾收集的分类:\n新生代收集 Minor GC / Young GC：指目标指示新生代的垃圾收集 老年代收集 Major GC / Old GC：指目标指示老年代的垃圾收集 混合收集 Mixed GC：整个新生代和部分老年代的垃圾收集 整堆收集 Full GC：收集整个 Java 堆和方法区的垃圾收集 标记-清楚算法 缺点:\n效率问题，标记和清除的过程效率都不高,随着对象数量的增长而降低 清除结束后会造成大量的碎片空间。有可能会造成在申请大块内存的时候因为没有足够的连续空间导致再次 GC。 标记-复制算法 复制算法的原理是，将内存分成两块，每次申请内存时都使用其中的一块，当内存不够时，将这一块内存中所有存活的复制到另一块上。然后将然后再把已使用的内存整个清理掉。\n缺点\n如果存活对象过多，可能会有大量的时间浪费复制上，所以这方法主要是针对存活率小的情况 空间浪费,将原来可用的空间压缩为原来的一半 因为新生代中有98%的对象都熬不过第一轮的回收，只有2%的对象可以存活下来。如果盲目将新生代划分为1 : 1的比例，就会浪费很多的空间。所以厂商们将新生代的布局划分为了一块较大的 Eden 空间和两块较小的 Survivor 空间。每次分配内存都是只是用 Eden 和其中一块 Survivor 空间。当 Minor 垃圾回收后，将存活的对象存放在保留的 Survivor 空间中，清空 Eden 和之前使用的 Survivor 空间。\n注:如果保留的survivor的空间不足够存放上一次垃圾收集下来的对象,这些对象便将通过分配担保机制直接进入老年代\n标记-整理算法. 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存\n在 GC 过程中移动存活对象，并更新所有引用这些对象的地方的数据，是一种极为负重的操作，而且移动操作必须暂停用户应用进程才能进行。这种暂停被描述为“ Stop The World”，也就是 STW。\n如果像标记-清除算法那样子完全不考虑移动和整理，Java 堆中的空间碎片问题将十分严重，只能依赖更复杂的内存分配器和内存访问器来解决。内存访问是用户程序中最频繁的操作，如果在此环节上添加额外的负担，势必会直接影响程序的吞吐量。\n那么移动会发生 STW，但是内存分配的时候更简单。直接清除会产生内存碎片，但是垃圾回收的时候更方便。无论是移动与否都有弊端。从垃圾回收的 STW 来看，直接清除的 STW 最短，甚至不用停顿，但从整个程序的吞吐量来看，移动对象更划算。不同的虚拟机实现厂商的注重点不同，他们的收集器也不一样。\n还有一种“和稀泥式”解决方案可以不在内存分配和访问上增加太大额外负担，做法是让虚拟机平时多数时间都采用标记-清除算法，暂时容忍内存碎片的存在，直到内存空间的碎片化程度已经大到影响对象分配时，再采用标记-整理算法收集一次，以获得规整的内存空间。前面提到的基于标记-清除算法的 CMS 收集器面临空间碎片过多时采用的就是这种处理办法。\nHotSpot的算法细节实现 根节点枚举 我们以可达性分析算法中从GC Roots集合找引用链这个操作作为介绍虚拟机高效实现的第一个例子。 固定可作为GC Roots的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如 栈帧中的本地变量表）中。尽管目标明确，但查找过程要做到高效并非一件容易的事情。\n目前，所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，因此根节点枚举与之前提及的整理内存碎片一样会有“Stop The World”的困扰。\n现在可达性分析算法耗时最长的查找引用链的过程已经可以做到与用户线程一起并发，但根节点枚举始终还是必须在一个能保障一致性的快照中才得以进行。\n这里“一致性”的意思是整个枚举期间执行子系统看起来就像被冻结在某个时间点上，不会出现分析过程中，根节点集合的对象引用关系还在不断变化的情况。否则分析结果的准确性无法保证\n在HotSpot的解决方案里，是使用一组称为OopMap的数据结构来达到这个目的。 一旦类加载动作完成的时候，HotSpot就会把对象内对应偏移量上的类型的数据计算出来，在即时编译过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。\n安全点 在OopMap的协助下，HotSpot可以快速准确地完成GC Roots枚举。 但一个很现实的问题随之而来：可能导致引用关系变化，或者说导致OopMap内容变化的指令非常多。 如果为每一条指令都生成对应的OopMap，那将会需要大量的额外存储空间，这样垃圾收集伴随而来的空间成本就会变得无法忍受的高昂。\n实际上HotSpot也并未对每条指令都生成OopMap，只是在“特定的位置”记录了这些信息，这些位置被称为安全点（Safepoint）。\n安全点的选取 有了安全点，因此用户程序执行时并非在任意位置都能停下来开始垃圾收集，而是强制要求必须执行到达安全点后才能够暂停。 因此，安全点的选定既不能太少，会让收集器等待时间过长；也不能太多，这会增大运行时的内存负荷。\n安全点位置的选取基本上是以“是否具有让程序长时间执行的特征”为标准 进行选定的。\n什么时候产生安全点： “长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转 等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点。\n如何跑到安全点 如何在垃圾收集发生时让所有线程（这里其实不包括执行JNI调用的线程）都跑到最近的安全点，然后停顿下来？这里有两种方案可供选择：抢先式中断（Preemptive Suspension）和主动式中断（Voluntary Suspension）。\n主动式中断 主动式中断的思想是当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。\n轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在Java堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象。\n由于轮询操作在代码中会频繁出现，这要求它必须足够高效。 HotSpot使用内存保护陷阱的方式，把轮询操作精简至只有一条汇编指令的程度。 当需要暂停用户线程时，虚拟机把对应的内存页设置为不可读，那线程执行到test指令时就会产生一个自陷异常信号，然后在预先注册的异常处理器中挂起线程实现等待，这样仅通过一条汇编指令便完成安全点轮询和触发线程中断了。\n最后说下抢先式，抢先式中断不需要线程的执行代码主动去配合，是系统介入，但现在几乎没有虚拟机实现采用抢先式中断来暂停线程响应GC事件。\n适合插入安全点的地方：\n方法（栈帧）结束前，但并不意味着一个方法只能有一个安全点 非计数循环末尾，避免循环体执行时间太长，导致长时间无法到达安全点 每条Java编译后的字节码边界 安全区域 安全区域可以理解是对安全点的存在问题的补充，上边说到线程会执行到附近的安全点停下来等待垃圾回收器介入处理，但如果线程没有执行呢，换句话就是说没有获得cpu执行权，比如某一个线程正在sleep或者等待磁盘输入，那么这个线程是不会走到安全点挂起自己的。\n这个时候就要引入安全区概念了，顾名思义，安全区就是一段指令域，在这个域中的指令不会对当前内存中的引用造成修改，当线程进入该区域后，会主动将自己的状态标记为“进入安全区”，这个时候如果发生gc，垃圾回收器发现该线程处于安全区域内，认为该线程不会对内存安全造成影响，便会跳过该线程，不会等待该线程到达安全点。 而线程在到达安全区边界时，同样也会检查当前gc是否在工作，如果gc正在工作，这个时候线程便会主动停下来，等待gc动作完成后再继续执行。\n虽然在一些特殊情况下，我们需要等待线程从休眠状态醒来才能进入安全点，但是通过使用安全区域，我们可以在更多的地方使线程响应停止-世界，从而提高JVM的性能和响应性。\n记忆集和卡集 记忆集是用来避免把整个老年代加进GC Roots扫描范围,事实上并不是只有新生代和老年代存在跨代引用这个问题\n记忆集是一种用来记录从非收集区域指向指向收集区域的指针集合的抽象数据结构,这个结构把老年代划分为若干个小块，标识出老年代哪一块内存会存在跨代引用。当发生 Minor GC 时，只有包含了跨代引用的小块内存中的老年代对象才会加入到 GC Roots 扫描中，避免整个老年代加入到 GC Roots 中。\n收集器只需要通过记忆集判断出某一块非收集区域是否存在有指向了收集区域的指针就可以了，并不需要了解这些跨代指针的全部细节。那设计者在实现记忆集的时候，便可以选择更为粗犷的记录粒度来节省记忆集的存储和维护成本，下面列举了一些可供选择（当然也可以选择这个范围以外的）的记录精度：\n字长精度：每个记录精确到一个机器字长（就是处理器的寻址位数，如常见的32位或64位，这个精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。 对象精度：每个记录精确到一个对象，该对象里有字段含有跨代指针。 卡精度：每个记录精确到一块内存区域，该区域内有对象含有跨代指针。 第三种精度的实现方式就是卡表,卡表和记忆集的关系就像接口和实现类一样\nHotSpot虚拟机定义的卡表只是一个字节数组。以下这行代码是HotSpot默认的卡表标记逻辑：\n1 CARD_TABLE [this address \u0026gt;\u0026gt; 9] = 0; 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在跨代指针，就将对应卡表的数组元素的值标识为 1，称为该元素变脏（Dirty），若无则标识为 0,在垃圾收集发生时,只要筛选出卡表中变脏的元素,就能轻易得出哪些卡页内存块中包含跨代指针,把它们加入GC Root\n写屏障 我们已经解决了如何使用记忆集来缩减GC Roots扫描范围的问题,但还没有解决卡表元素如何维护的问题\n写屏障（Write Barrier）可以看做在虚拟机层面对“引用类型字段赋值”动作的 AOP 切面，赋值前的写屏障称为“写前屏障（Pre-Write Barrier）”，赋值后的写屏障称为“写后屏障（Post-Write Barrier）”。\n应用写屏障后，虚拟机会为所有赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表操作，无论更新的是不是老年代对新生代的引用，每次只要对引用进行更新，就会产生额外的开销。\n伪共享问题 缓存一致性协议在计算机中针对的最小单元：缓存行，每个缓存行的大小是64字节，一串连续的64字节数据都会存储到缓存行中。\n假设数据A和数据B在同一缓存行中，CPU1修改了数据A，根据缓存一致性协议，CPU1会通知其他CPU这一行的缓存数据已经失效。此时CPU2想要修改数据B，但是缓存行已经失效了，所以需要重新从主内存中读取数据，然后重新写会缓存行中。这样缓存的优势就完全没有了。\n上述问题就是伪共享的场景，如果同时有多个CPU同时修改同一缓存行的数据，频繁回写主内存，会大大降低性能。\n解决方案\n伪共享的根源就是不同的数据缓存到了同一缓存行中，如果我们能把独立的数据都单独存储到不同的缓存行，那么伪共享的问题也就不存在了。\n缓存行填充：当我们存储的数据不足64字节的时候，我们可以手动将余下的字节空间填充，以空间换时间的方式，解决伪共享。\n为了解决伪共享问题,不采用无条件的写屏障,而是先检查卡表标志,只有当该卡表元素未被标记过时才将其标记未变脏\n并发的可达性分析 可达性分析算法理论上要求全过程都基于一个能保障一致性的快照中才能进行分析，这意味着必须全程冻结用户线程（Stop The World）。\n为什么必须在一个能保证一致性的快照上才能进行对象图的遍历呢？\n如果用户线程是冻结的，没问题。\n若用户线程没冻结，也就是用户线程与收集器并发工作呢？收集器在对象图标记，同时用户线程在修改引用关系（修改对象图的结构），这样可能出现两种后果：\n把原本消亡的对象错误标记为存活，这种情况虽不好（产生了浮动垃圾），但还可以容忍。 把原本存活的对象标记为消亡，这就很严重了，程序肯定会因此报错。 当且仅当两种情况下同时满足会产生对象消失的问题\n赋值器插入一条或多条从黑色对象到白色对象的新引用 赋值器删除了全部从灰色对象到该白色对象的直接或者间接引用 两种方案解决:\n增量更新:针对新增的引用，将其记录下来等待遍历，即增量更新 原始快照:如果期间发生变化，则可以记录起来，保证标记依然按照原本的视图来。 ","date":"2023-12-10T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/jvm%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%9A%84%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/","title":"JVM垃圾收集的算法及其实现"},{"content":"@TOC\njvm运行时数据区 线程隔离也就是线程不共享的意思\njava8内存结构图:\n程序计数器: 程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 在Java虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。\n由于Java虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\n如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）\n解释:当JVM将字节码文件读取到内存的时候,每一条字节码指令都用自己的内存区域,由于JVM的多线程是通过线程轮流切换的,当它从另一个线程的指令切换过来的时候,要继续执行当前线程的命令,此时就需要程序计数器来记录当前线程下一条执行的字节码文件的内存地址\n解答:为什么执行native方法,计数器为空?\n因为执行native方法时,java是通过调用jvm中的JNI来调用本地的c/c++库中的方法,这个方法不是java实现的,就不会产生相应的 字节码文件,并且c/c++执行时内存分配是由自己语言决定的\n唯一一个无OOM的区域是什么?\n无OOM就是没有内存溢出异常(OutOfMemoryError),在编译时已经确定该线程代码的偏移量最大值,根据该最大偏移量可以分配内存空间,在线程运行时，只改变程序计数器的值，不涉及空间的扩展,所以不会存在空间不够用的情况。\n因为java虚拟机规范了程序计数器是没有内存溢出的区域\njava虚拟机栈: 1.概述:\n由于跨平台性的设计，Java的指令都是根据栈来设计的。不同平台CPU架构不同，所以不能设计为基于寄存器的。优点是跨平台，指令集是8位对齐，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。Java虚拟机栈（Java Virtual Machine Stack），早期也叫Java栈。每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧（Stack Frame），对应着一次次的Java方法调用，是线程私有的。生命周期和线程一致，作用：主管Java程序的运行，它保存方法的局部变量、部分结果，并参与方法的调用和返回。\n该区域存在OutOfMemoryError异常,StackOverflowError异常;\n栈溢出的原因:\njava虚拟机规定允许栈的大小是动态的或者固定不变的\n如果采用固定大小的虚拟机栈,每个线程的java虚拟机栈容量可以在线程创建的时候独立选定,如果线程请求分配的栈容量超过java虚拟机栈允许的最大容量,java虚拟机会抛出一个StackOverflowError;\n如果Java虚拟机栈可以动态扩展，并且尝试扩展时无法申请到足够的内存，会抛出OutOfMemoryError（注意HotSpot虚拟机的栈容量是不可以动态扩展的）\n栈帧: 栈帧一般包括局部变量表,操作数栈,方法返回地址,动态链接,一些附加信息这五部分\n局部变量表:\n局部变量表所需的容量大小是在java源码编译期（即java代码被编译成Class文件时）确定下来的，并保存在方法的Code属性的maximum local variables数据项中。在方法运行期间不会改变局部变量表的大小的, 定义为一个数字数组,\n包括三种数据类型:8种基本数据类型+引用数据类型+returnAddress类型(指向下一条字节码指令的地址),\n如果当前栈帧是由构造方法或者实例方法(非static修饰的方法,静态方法中不能使用this)创建的那么该对象引用的this将会放在局部变量表index=0的位置,其余参数按照参数表顺序继续排列.\n注:\n局部变量表中的槽位是可以重复利用的,如果一个局部变量过了其作用域,那么在其作用域之后申明的新的局部变量就很有可能复用过期局部变量的槽位,从而达到节省资源的目的 局部变量表中的变量也是重要的垃圾回收根节点,只要被局部变量表直接或间接引用的对象都不会被回收 2.操作数栈:\na.在方法执行过程中,根据字节码指令,往栈中写入数据或提取数据,即入栈(push)/出栈(pop).某些字节码指令将值压入操作数栈,其余操作指令将操作数取出栈.使用它们后再把结果压入栈(出栈入栈由执行引擎执行).\nb.操作数栈主要用于保存运算过程中的的中间结果,同时作为计算过程中变量的临时的存储空间.操作数栈就是JVM执行引擎的一个工作区,当一个方法开始执行的时候,一个新的栈帧也会随之被创建出来,此时这个方法的操作数栈是空的.每一个操作数栈都会拥有一个明确的栈深度用于存储数值,其所需最大深度在编译期就定义好了,保存在方法的Code属性中,位max_stack的值.栈中的任一元素都可为任意Java类型(32bit的类型占用一个栈单位深度,64bit的类型占两个栈单位深度).操作数栈并非采用访问索引的方式来进行数据访问的,而是只能通过标准的入栈(push)和出栈(pop)操作来完成一次数据访问.如果被调用的方法有返回值,其返回值将会被压入当前栈帧的操作数栈中,并更新PC寄存器中下一条需要执行的字节码指令.操作数栈中元素的数据类型必须与字节码指令的序列严格匹配,这由编译器在编译期间进行验证,在类加载过程中的类检验阶段的数据流分析阶段要再次验证.Java虚拟机的解释执行引擎被称为\u0026quot;基于栈的执行引擎\u0026quot;,里面的栈就是操作数栈.\n3.动态链接:\n动态链接 方法返回地址 一些附加信息被叫做帧数据区\n每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接（Dynamic Linking）。比如：invokedynamic指令\n在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用（Symbolic Reference）保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。\n为什么需要常量池?\n字节码文件中需要很多数据的支持,但数据很大,不能直接保存到字节码文件中,所有常量池的作用就是为了提供一些符号喝常量,便于指令的识别\n方法的调用:解析和分派 在jvm中,将符号引用转换为调用方法的直接引用与方法的绑定机制相关\n静态链接: 当一个字节码文件被装载进JVM内部时，如果被调用的目标方法在编译期可知，且运行期保持不变时，这种情况下降调用方法的符号引用转换为直接引用的过程称之为静态链接\n动态链接: 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期将调用的方法的符号转换为直接引用，由于这种引用转换过程具备动态性，因此也被称之为动态链接。\n早期绑定 早期绑定就是指被调用的目标方法如果在编译期可知，且运行期保持不变时，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。\n晚期绑定 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法，这种绑定方式也就被称之为晚期绑定。\n虚方法和非虚方法 如果方法在编译期就确定了具体的调用版本,这个版本在运行是不可变的,这样的方法被称为非虚方法.静态方法、私有方法、final方法、实例构造器、父类方法都是非虚方法.其他方法称为虚方法.虚拟机调用方法普通调用指令如下:\ninvokeStatic 调用静态方法.解析阶段确定唯一方法版本. invokespecial 调用方法、私有及父类方法.解析阶段确定唯一方法版本. invokevirtual 调用所有虚方法以及final方法. invokeinterface 调用接口方法 动态调用指令:\n5.invokedynamic 动态解析出需要调用的方法并执行.使用Lamda表达式时会使用该指令\n分派: 在面向对象的编程中,会很频繁的使用到动态分派(存在继承关系的对象,在确定具体调用方法的版本的时候可能需要迭代往上找确定的方法版本,直到找到为止),如果每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响执行效率.因此为了提高性能,JVM采用在类的方法区建立一个需方发表(virtual method table)(非虚方法不会出现在表中)来实现,使用索引表来代替查找.每个类中都有虚方法表,表中存放着方法的实际入口.虚方发表会在类的加载的链接阶段被创建并初始化,类的变量初始值准备完成之后,JVM会把该类的虚方法表也初始完毕.\n方法返回地址: 方法退出后,必须返回到最初方法被调用时的位置,方法才能继续执行,方法返回时可能需要在栈帧中保存一些信息,用来帮助恢复它的上层主调方法的执行状态.一般来说,方法正常退出时,主调方法的PC计数器的值就可以作为返回地址(此地址就保存在方法返回地址中),栈帧中很可能会这个计数器值.而方法异常退出时,返回地址是要通过异常处理器表来确定的,栈帧中一般不会保存这部分信息.\n附加信息 Java虚拟机规范允许虚拟机实现增加一些规范里没有描述的信息到栈帧中.例如与调试、性能收集相关信息.\n堆: 堆的特点: 堆内存的大小是在jvm启动时可以指定的,但通常是在运行时动态分配的,这意味着堆内存可以根据程序的需要动态增长或缩小,以容纳新创建的对象 堆的存储在物理上是不连续的,但是在逻辑上是连续的 虽然堆是线程共有的,但是可以划分线程私有缓存区 对象和数组都不会存储在栈上,栈帧中保存的是引用,这个引用指向的是对象或者数组在堆中的位置,字符串池和类的静态变量放入java 堆中 在方法结束后,堆的对象不会立马被清除,仅仅只会在垃圾收集的时候才被移除 堆是垃圾回收的重点区域 新生代的垃圾回收;\n所有类都是在伊甸园被new出来的,当伊甸园的空间用完时,程序有需要创建对象,JVM的垃圾回收器将堆伊甸园进行垃圾回收,\n将伊甸园中不再被其他对象所引用的对象进行销毁,然后将伊甸园中剩余对象移动到幸存0区,若幸存0区也满了,再对该区进行垃圾回收,然后移动到1区,如果一区也满了,再移动到养老区,若养老区也满了,就会产生(fullGC),进行养老区的内存清理\n首先,当Eden区满的时候会触发第一次GC,把还活着的对象拷贝到From区,当Eden区再次触发GC的时候会扫描Eden区和From区域,对这两个区域进行垃圾回收,经过这次回收后还存活的对象,则直接复制到To区域(如果有对象的年龄已经达到了老年的标准,则赋值到老年代区),同时把这些对象的年龄+1,然后,清空Eden和From区中的对象,也即复制之后有交换,谁空谁是to最后,To区和From区互换,原To区成为下一次GC时的From区。部分对象会在From区和To区域中复制来复制去,如此交换15次(次数可以调节)最终如果还是存活,就存入到老年代\nMinor GC ： 清理年轻代 Major GC ： 清理老年代 Full GC ： 清理整个堆空间，包括年轻代和永久代 将对象根据存活概率进行分类，对存活时间长的对象，放到固定区，从而减少扫描垃圾时间及GC频率。针对分类进行不同的垃圾回收算法，对算法扬长避短\n为什么分代?\njdk8后变化,元空间取代了永久代,元空间使用本地内存(系统内存),永久代使用虚拟机堆中内存\n为什么废除永久代?\n永久代很难调整,其中类的元数据信息在每次fullFC的时候可能被收集,但成绩很难令人满意(回收效率偏低),\n给永久代分配多大的空间很难确定,因为永久代的大小依赖很多因素,比如JVM加载的class总数,常量池的大小,方法的大小,内存经常不够用发生内存泄漏\nMinorGc的过程就是:复制-\u0026gt;清空-\u0026gt;互换 在年轻代中使用的是MinorGC,这种GC采用的是复制算法\n方法区: 方法区的基本理解:\n方法区的大小决定了系统可以保存多少个类,如果系统定义了太多的类,导致方法区移除,虚拟机同意会抛出内存溢出错误 方法区被各个线程共享 大小可以固定也可以是动态扩展的 物理内存可以不连续 存储内容:\n字符串常量池为什么放到堆中? 因为永久代的回收效率很低,在full gc的时候才会触发,而full gc是老年代的内存不足,永久代的内存不足才会触发的\n这就导致回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆中能及时回收\n类型信息 对每个加载的类型（ 类class、接口interface、枚举enum、注解annotation），JVM必 .须在方法区中存储以下类型信息：\n这个类型的完整有效名称（全名=包名.类名） 这个类型直接父类的完整有效名（对于interface或是java. lang.Object，都没有父类） 这个类型的修饰符（public， abstract， final的某个子集） 这个类型直接接口的一个有序列表 域信息（成员变量） JVM必须在方法区中保存类型的所有域的相关信息以及域的声明顺序。 域的相关信息包括：域名称、 域类型、域修饰符（public， private， protected， static， final， volatile， transient的某个子集） 运行时常量池 运行时常量池主要存放在类加载后被解析的字面量与符号引用,但不止这些 运行时常量池具备动态性,可以添加数据,比较多的使用就是String的intern()方法 non-final的类变量 静态变量和类关联在一起，随着类的加载而加载，他们成为类数据在逻辑上的一部分 类变量被类的所有实例所共享，即使没有类实例你也可以访问它。 方法信息: 方法名称\n方法的返回类型（或void）\n方法参数的数量和类型（按顺序）\n方法的修饰符（public， private， protected， static， final， synchronized， native ， abstract的一个子集）\n方法的字节码（bytecodes）、操作数栈、局部变量表及大小（ abstract和native 方法除外）\n异常表（ abstract和native方法除外:每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址、被捕获的异常类的常量池索引\n直接内存 直接内存位于本地内存，不属于JVM内存，但是也会在物理内存耗尽的时候报OOM,所以也讲一下。\n在jdk1.4中加入了NIO（New Input/Putput）类，引入了一种基于通道（channel）与缓冲区（buffer）的新IO方式，它可以使用native函数直接分配堆外内存，然后通过存储在java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，这样可以在一些场景下大大提高IO性能，避免了在java堆和native堆来回复制数据。\n方法区的垃圾回收:\n一般来说这个区域的回收效果比较难令人满意,尤其是类的卸载,条件相当苛刻,但是这部分区域的回收有时又确实是必要的\n方法区的垃圾收集主要回收两部分:常量池中废弃的常量和不再使用的类型\n判断一个类是否还在使用的条件:\n该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。\n加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。\n该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。\nJava虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。\n废弃的常量和不再使用的类型\n判断一个类是否还在使用的条件:\n该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。\n加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。\n该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。\nJava虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。\n","date":"2023-11-26T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/jvm%E8%BF%90%E8%A1%8C%E6%97%B6%E5%86%85%E5%AD%98%E5%88%86%E5%B8%83/","title":"JVM运行时内存分布"},{"content":"@[ToC]\nHashMap的容量计算 HashMap为什么创建数组的时候是二次幂?\n当length为2的n次幂的时候会,hash\u0026amp;(length-1)和hash%length是一样的,而位运算相对来说更快\n1 2 3 4 5 6 7 8 9 10 //当传入容量时,将传入的数的0位转换为1 static final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1; n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 红黑树的平均查找长度是log(n) 链表的平均查找长度是n/2 当长度大于8的时候才有转换为树的必要,6也是但是转换树和生成树的时间并不会太短\n当数组长度大于64,并且链表中的节点的个数大于8\nHashMap的初始化 1 2 3 4 5 6 7 8 9 10 11 //存放元素的数组 transient Node\u0026lt;K,V\u0026gt;[] table; transient Set\u0026lt;Map.Entry\u0026lt;K,V\u0026gt;\u0026gt; entrySet; //记录元素的个数 transient int size; //记录map的修改次数 transient int modCount; //阈值,当实际大小(容器*负载因子)超过临界值时,会进行扩容 int threshold; //加载因子,用来衡量HashMap满的程度, size/capacity final float loadFactor; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } //给定指定初始容量的大小 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } //构造一个空的和默认负载因子 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } public HashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } public HashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) { // 使用默认加载因子 this.loadFactor = DEFAULT_LOAD_FACTOR; //调用PutMapEntries()来完成HashMap的初始化赋值过程 putMapEntries(m, false); } /** * 将传入的子Map中的全部元素逐个添加到HashMap中 */ final void putMapEntries(Map\u0026lt;? extends K, ? extends V\u0026gt; m, boolean evict) { // 获得参数Map的大小，并赋值给s int s = m.size(); // 判断大小是否大于0 if (s \u0026gt; 0) { // 证明有元素来插入HashMap // 判断table是否已经初始化 如果table=null一般就是构造函数来调用的putMapEntries，或者构造后还没放过任何元素 if (table == null) { // pre-size // 如果未初始化，则计算HashMap的最小需要的容量（即容量刚好不大于扩容阈值）。这里Map的大小s就被当作HashMap的扩容阈值，然后用传入Map的大小除以负载因子就能得到对应的HashMap的容量大小（当前m的大小 / 负载因子 = HashMap容量） // ((float)s / loadFactor)但这样会算出小数来，但作为容量就必须向上取整，所以这里要加1。此时ft可以临时看作HashMap容量大小 float ft = ((float)s / loadFactor) + 1.0F; // 判断ft是否超过最大容量大小，小于则用ft，大于则取最大容量 int t = ((ft \u0026lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 暂时存放到扩容阈值上,tableSizeFor会把t重新计算到2的次幂给扩容数组大小 if (t \u0026gt; threshold) threshold = tableSizeFor(t); } // 如果当前Map已经初始化,且这个map中的元素个数大于扩容的阀值就得扩容 // 这种情况属于预先扩大容量，再put元素 else if (s \u0026gt; threshold) // 后面展开说 resize(); // 遍历map,将map中的key和value都添加到HashMap中 for (Map.Entry\u0026lt;? extends K, ? extends V\u0026gt; e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); // 调用HashMap的put方法的具体实现方法putVal来对数据进行存放。该方法的具体细节在后面会进行讲解 // putVal可能也会触发resize putVal(hash(key), key, value, false, evict); } } } HashMap的put方法 put方法是比较复杂的,实现步骤如下:\n先通过hash值计算出来key映射到那个桶 如果桶上没有碰撞冲突,则直接插入 如果出现冲突了,则调用红黑树的方法插入数据,否则采用传统的链式方法插入,如果链的长度到达临界值,则把链转变为红黑树 如果桶中有重复的键,则把该键的value值替换 如果size大于阈值,则进行扩容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 //将指定的值与此映射中的指定键相关联。即当前key应该存放在数组的哪个下标位置 //如果映射先前包含键的映射，则替换旧的值。 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // 判断table是否为空 if ((tab = table) == null || (n = tab.length) == 0) // 如果空的话，会先调用resize扩容,resize我们后面展开讲解 n = (tab = resize()).length; // 把通过hash得到的值与数组大小-1进行与运算，这个运算就可以实现取模运算 // 得到key所对应的数组的节点，然后把该数组节点赋值给p,然后判断这个位置是不是有元素 if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) // key、value包装成newNode节点，直接添加到此位置。 tab[i] = newNode(hash, key, value, null); // 如果当前数组下标位置已经有元素了，又分为三种情况。 else { Node\u0026lt;K,V\u0026gt; e; K k; // 当前位置元素的hash值等于传过来的hash，并且他们的key值也相等 if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) // 则把p赋值给e，后续需要新值把旧值替换 e = p; // 来到这里说明该节点的key与原来的key不同，则看该节点是红黑树，还是链表 else if (p instanceof TreeNode) // 如果是红黑树，则通过红黑树的方式，把key-value存到红黑树中。 e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); else { // 把key-value使用尾插法插到链表尾 for (int binCount = 0; ; ++binCount) { // 遍历该链表，知道知道下一个节点为null。 if ((e = p.next) == null) { // 说明到链表尾部，然后把尾部的next指向新生成的对象 p.next = newNode(hash, key, value, null); // 如果链表的长度大于等于8 if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 则链表转化成为红黑树 后面再补充 treeifyBin(tab, hash); break; } // 如果在链表中找到了相同key的话，直接退出循环 if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } // 说明发生了碰撞，e代表的是旧值，需要替换为新值 if (e != null) { // existing mapping for key V oldValue = e.value; // 判断是不是允许覆盖旧值，和旧值是否为空 if (!onlyIfAbsent || oldValue == null) // 把旧值替换 e.value = value; // 空实现 afterNodeAccess(e); // 返回新值 return oldValue; } } // fail-fast机制每次对结构改变进行+1 ++modCount; // 判断HashMap中的存的数据大小，如果大于数组长度*0.75，就要进行扩容 if (++size \u0026gt; threshold) resize(); // 也是一个空的实现 afterNodeInsertion(evict); return null; } HashMap的resize()方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 final Node\u0026lt;K,V\u0026gt;[] resize() { //将扩容前的数组拿到 Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果oldCap大于0代表着oldCap已经resize()一次过 if (oldCap \u0026gt; 0) { if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } //如果oldThr\u0026gt;0但是oldCap\u0026lt;0 代表这是初始化后第一次resize() else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults //无参构造 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } //计算新的阈值,之前的阈值为容器的容量 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; //进行拷贝 if (oldTab != null) { for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null)yuuan newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { // preserve order //定义两个链表,就数组的元素要么留在原位置,要么等于原位置+oldCap Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; //高位是0留在原位 if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } //高位是1去新位置 else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } ","date":"2023-11-12T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/hashmap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","title":"HashMap源码分析"},{"content":"@TOC\n继承关系 1 2 3 4 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt;` implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable{ } ​ ArrayList的底层是数组队列,相当于动态数组.与java中的数组相比,它的容量能动态增加.\nRandomAccess是一个标志接口,表名实现这个接口的List集合是支持快速随机访问的,在ArrayList中,我们可以通过元素的索引快速获取元素对象,这就是快速随机的访问\nArrayList 实现了 Cloneable 接口，即覆盖了函数clone()，能被克隆。\n一个类要想克隆,只能实现这个接口\n1 2 3 4 5 6 7 8 9 10 11 public Object clone() { try { ArrayList\u0026lt;?\u0026gt; v = (ArrayList\u0026lt;?\u0026gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // 这个异常一般不会发生 throw new InternalError(e); } } ArrayList 实现了 java.io.Serializable接口，这意味着ArrayList支持序列化，能通过序列化去传输。\n继承AbstractList该类提供了List接口的骨架实现\n有参构造 1 2 3 4 5 6 7 8 9 10 public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } 1 2 3 4 5 6 7 8 9 10 11 12 public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { //将构造方法种的集合参数转换为数组 elementData = c.toArray(); if ((size = elementData.length) != 0) { //如果不是object数组要转换为object数组 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { //把空数组赋值给elementData this.elementData = EMPTY_ELEMENTDATA; } } copy方法\n1 2 3 4 5 6 7 8 9 10 public static \u0026lt;T,U\u0026gt; T[] copyOf(U[] original, int newLength, Class\u0026lt;? extends T[]\u0026gt; newType) { //三元运算符判断返回数组的属性,但结果不论如何都会创建一个新的数组 T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); //数组的拷贝 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } 无参构造 1 2 3 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } 1 2 private static final Object[] EMPTY_ELEMENTDATA = {}; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; 为什么ArrayList底层要提供两个空数组\nDEFAULTCAPACITY_EMPTY_ELEMENTDATA数组是你无参构造的时候创建的空数组\nEMPTY_ELEMENTDATA是你指定容量为0创建的空数组\nadd方法 1 2 3 4 5 6 7 8 9 public void add(int index, E element) { //检查索引是否合法 rangeCheckForAdd(index); ensureCapacityInternal(size + 1); System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 1 2 3 4 5 6 7 public boolean add(E e) { //检查容量 ensureCapacityInternal(size + 1); elementData[size++] = e; //底层直接返回的true return true; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 //计算集合的容量 private static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } private void ensureCapacityInternal(int minCapacity) { //计算现在准确的大小 ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private void ensureExplicitCapacity(int minCapacity) { //统计实际修改集合的次数,用来维护线程安全 modCount++; //如果所需的元素大小大于集合的长度(容量不够的情况下),扩容 if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } 扩容\n1 2 3 4 5 6 7 8 9 10 11 12 13 private void grow(int minCapacity) { //计算之前的数组大小 int oldCapacity = elementData.length; //扩容1.5倍作为新数组的大小 int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //如果扩容后比需要的还小,就把所需的大小当作新的数组大小 if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; //如果大于定义的数组的最大长度,MAX_ARRAY_SIZE长度是int的最大值减8,应该我们是用int类型的size变量去维护的元素的个数,还有数组除了储存数据本身还需要最大32bytes的大小去对象头 if (newCapacity - ,MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); } 对象头包括：\nMark Word：用于对象自身的运行时数据存储，如HashCode，GC分代年龄，锁状态标志，线程持有的锁，偏向线程ID和偏向时间戳等； Klass Pointer：对象指向它类元数据的指针，JVM通过这个指针长度来确定对象是哪个类的实例。 数组长度（只有数组对象才有）：记录数组对象的长度。\nset方法 1 2 3 4 5 6 7 8 public E set(int index, E element) { //检验参数是否大于size rangeCheck(index); //根据索引取出元素 E oldValue = elementData(index); elementData[index] = element; return oldValue; } get方法 1 2 3 4 public E get(int index) { rangeCheck(index); return elementData(index); } 迭代器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public Iterator\u0026lt;E\u0026gt; iterator() { return new Itr(); } private class Itr implements Iterator\u0026lt;E\u0026gt; { int cursor; //光标,开始指向0 int lastRet = -1; //将集合实际修改次数赋值给预期修改次数 int expectedModCount = modCount; Itr() {} public boolean hasNext() { //判断光标对应的地方是否有元素 return cursor != size; } @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public E next() { //检查修改次数是否改变,防止并发问题 checkForComodification(); int i = cursor; //如果大于元素个数,则说明没有元素了 if (i \u0026gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; //如果大于集合的长度,则是产生了并发修改异常 if (i \u0026gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; //将lastRet赋值为这次访问元素的下标 return (E) elementData[lastRet = i]; } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } remove方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //集合删除元素的方法 public boolean remove(Object o) { //判断要删除的元素是不是null if (o == null) { for (int index = 0; index \u0026lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u0026lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } ​\n1 2 3 4 5 6 7 8 9 10 11 //真正删除元素的方法 private void fastRemove(int index) { //真正修改集合的次数会递增 modCount++; int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); //使用垃圾回收机制清除 elementData[--size] = null; } 注:当要删除的元素在集合的倒数第二个位置的时候,不会产生并发修改异常\n原因: 是因为在调用hasNext()方法的时候,光标的值和集合的长度一样,那么就会返回false,因此就不会再去调用next方法获取集合的元素,既然不会调用next方法那么底层就不会产生并发异常\n迭代器中的remove方法 用处: 删除此迭代器返回的最后一个元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public void remove() { //lastRet记录迭代器最后访问的元素,初始化为-1,小于零代表该迭代器没用访问元素 if (lastRet \u0026lt; 0) throw new IllegalStateException(); //坚持并发问题 checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; //重新为expectModCount赋值,就不会产生并发问题 expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } clear方法 1 2 3 4 5 6 7 public void clear() { modCount++; for (int i = 0; i \u0026lt; size; i++) elementData[i] = null; size = 0; } contain方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public boolean contains(Object o) { return indexOf(o) \u0026gt;= 0; } public int indexOf(Object o) { if (o == null) { for (int i = 0; i \u0026lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i \u0026lt; size; i++) if (o.equals(elementData[i])) return i; } return -1; } 常见问题 ArrayList是如何扩容的?\n第一次扩容为10,之后扩容为当前容量的1.5倍\nArrayList频繁扩容导致添加性能极具下降,如何解决\n构造具有大的初始容量的空列表 ensureCapacity()方法指定最小容量,进行扩容 ArrayList插入或者删除元素一定比LinkedLIist慢吗?\n不一定\nArrayList是不是线程安全的?\n线程不安全\n","date":"2023-10-09T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/arraylist%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","title":"ArrayList源码分析"},{"content":"@TOC\n创建对象的方式: new关键字 反射 clone 反序列化 创建对象的流程: 检查类是否已经被加载\n去常量池中查找该引用所指向的类是否被虚拟机加载,如果没有被加载,就进行类的加载,类的加载包括 加载,链接,初始化,对象的大小在类加载完成时确定\n为对象分配内存空间\n为对象分配空间,即把一块确定大小的内存块从java堆中划分出来\n指针碰撞:假设java堆中的内存是绝对规整的,用过的内存放到一遍,没用过的放到一遍,中间放着一个指针作为分界点的指示器,那所分配的大小和执指针向空闲空间方向挪动一段与对象大小相等的距离 已使用的内存和空闲的内存象胡交错,那就没办法简单地进行指针碰撞了,虚拟机就必须维护一个列表,记录那些内存块是可用的,在分配的时候从列表中找到一块足够大的空间划分给对象实例,并更新列表上的记录,这种分配方式称为空闲列表\n选择哪种分配方式由java堆是否规整决定,而java堆是否规整又由所采用的垃圾收集器是否带有压缩功能决定\n分配内存时的并发安全问题\n产生原因:\n对象创建在虚拟机中是非常频繁地行为，仅仅是修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。\n解决方案:\n对分配内存空间的动作进行同步处理-实际上虚拟机采用的CAS配上失败重试的方式更新操作的原子性 把内存分配的动作按照线程划分到不同的空间之中进行,即每个线程再java堆中预先分配一小块内存,称为本地线程分配缓存,哪个线程要分配内存,就在哪个线程的本地缓存区中分配,只有本地缓存区用完了,分配新的缓存区的时候才需要同步锁定 3.初始化内存\n内存分配完成之后，虚拟机必须将分配到的内存空间（但不包括对象头）都初始化为零值。零值初始化意思就是对对象的字段赋0值，或者null值，这也就解释了为什么这些字段在不需要进程初始化时候就能直接使用。 如果使用了TLAB的话，这一项工作也可以提前至TLAB分配时顺便进行。\n4.设置对象头\n接下来 ,虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息，这些信息存放在对象的对象头(Object Header ) 之中。 根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\n5.执行init方法\n在上面工作都完成之后,从虚拟机的视角来看,一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚刚开始——\u0026lt; init \u0026gt;方法（实例初始化方法，类初始化方法是\u0026lt; clinit \u0026gt; ）还没有执行，所有的字段都还为零值。 所以，一般来说(由字节码中是否跟随invokespecial指令所决定，不走构造器的初始化方式没有这条指令)，执行new指令之后会接着 执行\u0026lt; init \u0026gt;方法（子类的\u0026lt; init \u0026gt;方法中会首先对父类\u0026lt; init \u0026gt;方法的调用），把对象按照程序员的意愿进行初始化。然后将内存地址赋给栈内存中的变量，这样一个真正可用的对象才算完全产生出来。\ninit方法和clinit方法的区别\ninit是实例对象构造器方法,clinit是类构造器方法\ninit 将语句块、变量初始化、调用父类的构造器等操作放到该方法中，顺序为：\n父类变量初始化块/父类语句块（按代码编写顺序） 父类构造函数。 子类变量初始化块/子类语句块（按代码编写顺序） 子类构造函数。 clinit 将静态语句块、静态变量初始化等操作放到该方法中，顺序为：\n父类静态变量初始化/父类静态语句块（按代码编写顺序） 子类静态变量初始化/子类静态语句块（按代码编写顺序） 若父类为接口,则不会调用父类的clinit方法,一个类可以没用clinit方法\nclinit方法是在类加载过程中执行的,而init是在对象实例化执行的,索引clinit一定比init先执行\n对象的内存分布 在java虚拟机中,对象在java内存中的存储布局可分为三块:\n对象头区域\n对象头区域包括两类信息:\n第一类是用于存储对象自身的运行时数据,如hash,GC分代年龄,锁状态标志,线程持有的锁,偏向线程id,偏向时间戳,\n该部分数据被设计成1个 非固定的数据结构 以便在极小的空间存储尽量多的信息（会根据对象状态复用存储空间）\n第二类是类型指针,即对象指向它的类型元数据的指针,java虚拟通过这指针来确定该对象是哪个类的实例(并不是所用的虚拟机实现都必须在对象数据上保留类型指针)\n注:如果对象是数组的话,那么对象头中还必须有一块用于记录数组长度的数据,因为不同对象虚拟机可以通过对象的元数据来确定对象的大小,但是从数组的元数据中却无法确定数组的大小\n实例数据\n是对象真正存储的有效信息，即类中定义的各种类型属性（包括从父类继承下来的和本身定义的）。\n实例数据存放具有一定规则：\n相同宽度的字段总是被分配在一起； 父类中定义的变量会出现在子类之前； 如果CompactFields参数为true（默认为true）子类的窄变量可能插入到父类变量的空隙 对齐填充\n为什么要进行对齐填充?\n64位系统CPU每次能处理8字节的数且只能以0x00000000 - 0x00000007,0x00000008-0x0000000f这样访问内存地址，不能0x00000002 - 0x00000009这样访问(为什么？因为硬件不允许，否则就不需要对齐填充了)。 那么如果没有对齐填充就可能会存在数据跨内存地址区域存储的情况。\n举个栗子：\n比如现在有4个数据,boolean,int,char,long，内存起始地址为0x00(简写)。\n在没有对齐填充的情况下，内存地址存放情况如下：\n因为处理器只能0x00-0x07，0x08-0x0F这样读取数据，所以当我们想获取这个long型的数据时，处理器必须要读两次内存，第一次(0x00-0x07)，第二次(0x08-0x0F)，然后将两次的结果才能获得真正的数值。\n那么在有对齐填充的情况下，内存地址存放情况是这样的：\n现在处理器只需要直接一次读取(0x08-0x0F)的内存地址就可以获得我们想要的数据了。\n对象的访问定位 jvm是如何通过栈帧中的对象引用访问到其内部的对象实例的?\n1.句柄访问\n优点:reference中存储的是稳定句柄地址,在对象被移动的时候(垃圾回收时,移动对象是非常普通的行为),只需要改变句柄池中的实例数据指针,而reference是不需要修改的\n2.直接指针(Hotspot采用)\n优点:访问速度快,节省了一次指针定位的时间开销,由于对象访问在java中非常频繁,因此这类开销积少成多也是一项极为可观的执行成本\n如何判断对象的生死 java堆中几乎存放着所有的对象实例,垃圾收集器在对堆进行回收前,第一件事就是确定这些对象哪些是活的,哪些是死的\n1.引用计数法\n在对象中添加计数器,引用一次+1,有一个引用销毁就-1,直到计数器中的数值为0,该对象死亡\n优点:简单高效 缺点:如果存在对象直接相互循环引用,那么他们的计数器就永远不会是0\n2.可达性分析算法\n通过一系列称为GC Roots的根对象作为起始节点集,从这些节点开始,根据引用关系向下搜索,搜索过程所走的路径称为引用链\n,如果这个对象到GC Roots间没有任何引用链相连,则证明此对象不可能再被使用\nGC Roots的对象包括以下几种:\n在虚拟机栈(栈帧中本地变量表)中引用的对象,例如当前正在运行的方法所使用到的参数,局部变量,临时变量 在方法区中类静态变量属性引用的对象 在方法区中常量引用的对象 在本地方法栈中JNI引用的对 象 java虚拟机内部的引用 所有被同步锁持有的对象 反映java虚拟机内部情况的JMXBean,JVMTI中注册的回调,本地方法代码缓存等 对象的五种引用:\n强引用:是指在程序代码中普遍存在的引用赋值,无论任何关系下,只要强引用关系还在,垃圾收集器就永远不会回收掉被引用的对象 软引用:是用来描述一些还有用,但非必须的对象,只被软引用关联着的对象,在系统将要发生内存溢出异常前,会把这些对象列进,回收范围之中进行第二次回收,如果这次回收还没有足够的内存,才会抛出内存溢出异常 弱引用也是用来描述哪些非必要对象,但是它的强度比软引用更弱一些,被软引用的关联的对象只能生存到下一次垃圾收集发生为止 虚引用:它是最弱的一种引用关系,一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,也无法通过虚引用来取得一个对象,为一个对象设置虚拟用关联的唯一目的只是为了能在这个都西昂被垃圾收集时收到一个系统通知,,虚引用在使用的过程中必须要和引用队列联合使用 终结器引用: 使用FinalReference引用对象，这种引用方式为终结器引用。需要结合引用队列来使用。finalize方法的调用就使用到了终结器引用。当一个对象正在被回收时（还没有回收），虚拟机会创建该对象的终结器引用，并且进入引用队列，在引用队列上有一个优先级很低的线程，扫描到终结器引用，会根据引用找到对象，调用对象的finalize方法，实现资源清理，在下次GC时该对象会被回收。由于使用finalize的对象在被回收时，需要两次,且调用finalize的线程优先级特别低，finalize方法可能迟迟不能调用，所以不推荐使用finalize方法 ","date":"2023-09-28T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/java%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E5%8F%8A%E7%94%9F%E6%AD%BB/","title":"Java对象的创建及生死"},{"content":"反射: 反射:将类的各个组成部分封装为其他对象,这就是反射机制\n好处:\n​\t1.可以再程序运行过程中,操作这些对象\n​\t2.可以解耦,提高程序的可扩展性\n反射的优缺点:\n优:反射提高了程序的灵活性和扩展性，降低耦合性，提高自适应能力。它允许程序创建和控制任何类的对象，无需提前硬编码目标类\n缺:\n1.性能问题 使用反射基本上是一种解释操作，用于字段和方法接入时要远慢于直接代码。因此Java反射机制主要应用在对灵活性和扩展性要求很高的系统框架上,普通程序不建议使用。\n2.使用反射会模糊程序内部逻辑\n程序人员希望在源代码中看到程序的逻辑，反射等绕过了源代码的技术，因而会带来维护问题。反射代码比相应的直接代码更复杂。\n3.安全限制\n使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如Applet，那么这就是个问题了。\n4.内部暴露\n由于反射允许代码执行一些在正常情况下不被允许的操作（比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用－－代码有功能上的错误，降低可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化。\n获取class对象的方式\nClass.forname(\u0026ldquo;全类名\u0026rdquo;) :将字节码文件加载进内存,返回Class对象(多用于配置文件,将类名定义再配置文件中.读取文件,加载类)\n类名.class:通过类名的属性class获取(多用于参数的传递)\n对象.getClass():getClass()方法再object类中定义\n同一个字节码文件在一次运行过程中,只会被加载一次,不论通过那一种方式获取的class对象都是同一个\nclass对象的功能: 1.获取成员变量:\n1 2 3 4 5 6 7 Filed[] getFileds() Filed[] getFileds(String name) 只能获取有public修饰的成员变量 Filed[] getDeclaredFileds() Filed[] getDeclaredFileds(String name) 获取所有的成员变量不考虑修饰符 获取成员变量后可以通过get(object obj) set(object obj ,object value )方法来进行设置和获取, setAccessible(true) 忽略访问权限修饰符的安全检查\n2.获取构造方法\n1 2 3 4 Constructor\u0026lt;T\u0026gt; getConstructor(类\u0026lt;?\u0026gt;... parameterTypes) Constructor\u0026lt;?\u0026gt;[] getConstructors() Constructor\u0026lt;T\u0026gt; getDeclaredConstructor(类\u0026lt;?\u0026gt;... parameterTypes) Constructor\u0026lt;?\u0026gt;[] getDeclaredConstructors() T newInstance()创建对象,用那种构造器就要再括号中满足那种构造器的参数要求,使用工造器对象来进行调用\n可以用过class对象.newInstance()来创建空参对象\n3.获取方法:\n1 2 3 4 Method getMethod(String name, 类\u0026lt;?\u0026gt;... parameterTypes) Method[] getMethods() Method getDeclaredMethod(String name, 类\u0026lt;?\u0026gt;... parameterTypes) Method[] getDeclaredMethods() method对象的执行方法:\n使用方法对象.invoke方法来进行调用类中方法执行\n","date":"2023-09-24T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/java%E5%8F%8D%E5%B0%84/","title":"Java反射"},{"content":"Redis 特征:\n键值型:value支持多种不同数据结构,功能丰富 单线程,每个命令具有原子性 低延迟,速度快(基于内存,io多路复用,良好的编码) 支持数据持久化 支持主从集群,分片集群 支持多语言客户端 数据结构 redis是一个key-value的数据库,key一般是string类型,不过value的类型多种多样\nString hello world Hash {name:\u0026ldquo;zjx\u0026rdquo;,age:21} List [A-\u0026gt;B-\u0026gt;C-\u0026gt;C-\u0026gt;] Set {A,B,C} SortedSet {A:1,B:2,C:3} GED {A:(120.3,30.5)} BitMap 0110110101110101011 HyperLog 0110110101110101011 redis的常用命令 keys :查看符合模板的所有key del 删除指定的一个或者多个key(如果多个key中存在不存在的key值则只删除存在的key) exists :判断key是否存在 expire :给一个key设置有效期,有效性到期时该key会被自动删除 ttl 查看还剩多少存在时间(-1代表永久有效) String类型 string类型,也就是字符串类型,是redis中最简单的储存类型\n其value是字符串,不过根据字符串的格式不同,又可以分为3类\nstring:普通字符串类型\nint:整数类型,可以做自增,自减操作\nfloat:浮点类型 ,可以做自增,自减操作\n不管是哪种格式,底层都是字节数组形式存储,只不过是编码方式不同.字符串类型的最大空间不能超过512m\nstring类型的常见命令\nset:添加和修改键值对 get:根据key值获取value mset:批量添加键值对 mget:批量根据key值查询value incr:让一个整形的key自增1 incrby:让一个整形的key自增指定步长 incrbyflaot让一个浮点型的数字自增并指定步长 setnx:添加一个string类型的键值对,前提是这个key不存在,否则不执行 setex:添加一个string类型的键值对,并且指定有效期 Key的层级结构 redis的key允许多个单词形成层级接口,多个单词之间用:隔开,格式如下:\n​ 项目名:业务名:类型:id\n这个格式也并非固定的,也可以根据自己的需求来删除或增加词条\nHash类型 Hash类型,也叫散列,其value是一个无序字典,类似于java中的HashMap结构\nstring结构是将对象序列化为json字符串后储存,当需要修改对象某个字段时不方便\nHash类型的常用命令有:\nhset key field value :添加或者修改hash类型key的field的值 hget key field hmset :批量添加多个hash类型key的field的值 hmget:批量获取多个hash类型key的field值 hgetall:获取一个hash类型的key中的所有field和value hkeys :获取一个hash类型的key中的所有filed hvals:获取一个hash类型的key的key中的所有value hincrby:让一个hash类型key的字段值自增并指定步长 hsetnx:添加一个hash类型的key的filed值,前提是这个field不存在,否则不执行 List类型 redis中的list和java中的linkedList类似,可以看作一个双向链表结构.既可以支持正向检索也可以支持反向检索\n特征和linkendlist类似:\n有序 元素可以重复 插入和删除快 查询速度一般 lpush key element\u0026hellip;:向列表左侧插入以一个或多个元素\nlpop key: 移除并返回列表左侧的第一个元素,没有则返回nil\nrpush key element..:向列表右端插入一个或多个元素\nrpop key:移除并返回右端的第一个元素\nlrange key star end: 返回一段角标的所有元素\nblpop和brpop :在指定时间内进行 和上面相似,只不过没有元素时等待指定时间,而不是直接返回nil\nllen key:获取list的长度\nlindex :获取指定索引的元素\nSet类型 redis的set结构于java中的hashset类似,可以看作一个value位bull的hashmap,应为也是一个hash表,因此具备于hashset类似的特性\n无序\n元素不可重复\n查找快\n支持交集,并集,差集等新功能\nsadd key member..:向set中增加一个或者多个元素\nsrem key member..:移除set中的指定元素\nscard key 返回set中元素的个数\nsismember key memeber :判断一个元素是否存在于set中\nsmembers :获取set的所有元素\nsinter key1 key:求交集\nsdiff key1 key2 求差集(key1中key2没有的)\nsunion key1 key2 求并集\nSortedSet类型 redis的sortedset是一个可排序的set集合,和java中的treeset类似,但底层数据结构却差别很大,sortedset中的没一个元素都带有一个score属性,可以基于score属性对元素排序,底层的实现是一个跳表加hash表(排序的命令z后面加个rev是降序)\n可排序\n元素不重复\n查询速度快\nzadd key score member..:添加一个或者多个sortedset,如果已经存则更更新其score值\nzrem key member :删除sorted set中的一个指定元素\nzscore key member :获取集合中的指定元素的score值\nzrank key member :获取集合中指定元素的排名\nzcard key: 获取集合中的元素个数\nzcount key min max 统计score值在给定范围内的所有元素的个数\nzincrby key increment memeber :让指定元素增长指定步长\nzrange key min max 按照score排序后,获取指定排名范围内的元素\nzrangebyscore key min max:按照score排序后,获取指定score范围内的元素\nzdiff zinter zunion 求差集,交集,并集\njedis的使用 连接redis数据库,然后使用jedis对象去执行命令\n1 2 3 Jedis jedis = new Jedis(\u0026#34;39.106.44.184\u0026#34;,6379); jedis.auth(\u0026#34;123456\u0026#34;); jedis.select(0); jedis的配置\n1 2 3 4 5 6 7 8 9 @Bean public Jedis jedis(){ JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMinIdle(0); jedisPoolConfig.setMaxIdle(8); jedisPoolConfig.setMaxTotal(8); JedisPool jedisPool = new JedisPool(jedisPoolConfig,\u0026#34;39.106.44.184\u0026#34;,6379,1000,\u0026#34;123456\u0026#34;); return jedisPool.getResource(); } redisTemplate的两种序列化实践方案\n方案一:\n自定义配置redisTemplate\nredisTemplate配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); Jackson2JsonRedisSerializer\u0026lt;Object\u0026gt; objectJackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer\u0026lt;Object\u0026gt;(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.activateDefaultTyping(LaissezFaireSubTypeValidator.instance,ObjectMapper.DefaultTyping.NON_FINAL); objectJackson2JsonRedisSerializer.setObjectMapper(objectMapper); template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); template.setValueSerializer(objectJackson2JsonRedisSerializer); template.setHashValueSerializer(objectJackson2JsonRedisSerializer); return template; } } 方案二:\n使用stringRedisTemplate 写入redis时,手动把对象序列化位json 读取redis时,手动把读取的json反序列化位对象 ","date":"2023-07-17T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/redis/","title":"Redis"},{"content":"什么是拦截器？\nSpring MVC中的拦截器（Interceptor）类似于Servlet中的过滤器（Filter），它主要用于拦截用户请求并作相应的处理。例如通过拦截器可以进行权限验证、记录请求信息的日志、判断用户是否登录等。\n拦截器的作用:\n拦截器可以说相当于是个过滤器：就是把不想要的或不想显示的内容给过滤掉。拦截器可以抽象出一部分代码可以用来完善原来的方法。同时可以减轻代码冗余，提高重用率。\n拦截器执行流程：\n（1）、程序先执行preHandle()方法，如果该方法的返回值为true，则程序会继续向下执行处理器中的方法，否则将不再向下执行；\n（2）、在业务处理器（即控制器Controller类）处理完请求后，会执行postHandle()方法，然后会通过DispatcherServlet向客户端返回响应\n（3）、在DispatcherServlet处理完请求后，才会执行afterCompletion()方法\n1.拦截器的配置 创建一个类实现handlerInterceptor接口,并重写其中的方法,按照需求进行相应方法的重写,并且preHandle方法的返回值为true时,表示放行后面的方法才会被执行,为false则被拦截\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Interceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return HandlerInterceptor.super.preHandle(request, response, handler); } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { HandlerInterceptor.super.postHandle(request, response, handler, modelAndView); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { HandlerInterceptor.super.afterCompletion(request, response, handler, ex); } } xml配置 在xml文件中创建拦截器对象放入ioc容器中管理,并指明拦截器的拦截器路径和放行路径\n1 2 3 4 5 6 7 \u0026lt;mvc:interceptors\u0026gt; \u0026lt;mvc:interceptor\u0026gt; \u0026lt;mvc:mapping path=\u0026#34;/**\u0026#34;/\u0026gt; \u0026lt;mvc:exclude-mapping path=\u0026#34;/home\u0026#34;/\u0026gt; \u0026lt;bean id=\u0026#34;Interceptor\u0026#34; class=\u0026#34;com.letstalk.config.Interceptor\u0026#34;/\u0026gt; \u0026lt;/mvc:interceptor\u0026gt; \u0026lt;/mvc:interceptors\u0026gt; 2.拦截器的执行顺序 在执行处理器的过程中,springmvc会现根据请求获取一个执行链HandlerExecutionChain类型的都对象,其中包含了handler这个处理器方法和interceptorList是一个拦截器集合,其中包含了springmvc默认自带的拦截器和自己所配置的拦截器并按照在集合中的顺序是,默认的在前面,自己配置的按照配置的先后设置顺序 (1)preHandle方法的执行顺序 可以看出通过for循环来执行preHandle,执行成功后interceptorIndex加一,也就代表了interceptorIndex执行最后以一个拦截器方法执行成功的拦截器在集合中的索引, for循环从0开始所以preHandle方法的执行顺序按照配置的前后顺序\n1 2 3 4 5 6 7 8 9 10 11 boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception { HandlerInterceptor[] interceptors = this.getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for(int i = 0; i \u0026lt; interceptors.length; this.interceptorIndex = i++) { HandlerInterceptor interceptor = interceptors[i]; if (!interceptor.preHandle(request, response, this.handler)) { this.triggerAfterCompletion(request, response, (Exception)null); return false; } } } 并在执行PreHandle方法后执行handler方法(处理器方法),并返回个modelandview类型的对象 (2) postHandle方法的执行顺序 从方法中可以看到for循环中倒序执行postHandle方法,所以postHandle方法的执行顺序是倒序\n1 2 3 4 5 6 7 8 9 10 void applyPostHandle(HttpServletRequest request, HttpServletResponse response, @Nullable ModelAndView mv) throws Exception { HandlerInterceptor[] interceptors = this.getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for(int i = interceptors.length - 1; i \u0026gt;= 0; --i) { HandlerInterceptor interceptor = interceptors[i]; interceptor.postHandle(request, response, this.handler, mv); } } } 从图可知posthandle方法在执行完成处理器方法之后 (3)afterCompletion方法的执行顺序 可以看出afterCompletion的执行顺序是从interceptorIndex开始倒序,也就是最后执行成功的拦截器开始倒序到第一个拦截器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void triggerAfterCompletion(HttpServletRequest request, HttpServletResponse response, @Nullable Exception ex) throws Exception { HandlerInterceptor[] interceptors = this.getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for(int i = this.interceptorIndex; i \u0026gt;= 0; --i) { HandlerInterceptor interceptor = interceptors[i]; try { interceptor.afterCompletion(request, response, this.handler, ex); } catch (Throwable var8) { logger.error(\u0026#34;HandlerInterceptor.afterCompletion threw exception\u0026#34;, var8); } } } } afterCompletion方法执行在视图processDispatchResult方法之后,也就是视图被数据渲染之后 ","date":"2023-06-16T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/springmvc%E6%8B%A6%E6%88%AA%E5%99%A8/","title":"Springmvc拦截器"},{"content":"1.springmvc的核心组件 以下是 Spring MVC 的核心组件： 1.DispatcherServlet(前端控制器）：\n定义:DispatcherServlet 是 Spring MVC 的前端控制器（Front Controller），它作为应用程序的入口点，负责处理所有的请求和响应。 作用:DispatcherServlet 根据请求的 URL 和配置的处理器映射，将请求分派给适当的处理器（Controller）进行处理，并获取处理结果。\n2.HandlerMapping（处理器映射）：\nHandlerMapping 将请求映射到相应的处理器（Controller）进行处理。 Spring MVC 提供了多种 HandlerMapping 的实现，包括注解驱动的 RequestMappingHandlerMapping、基于接口的 BeanNameUrlHandlerMapping 等。 Controller（控制器：\n3.Controller 是处理请求并返回响应的组件。\n在 Spring MVC 中，可以使用多种方式定义 Controller，包括使用注解标记的控制器类、实现特定接口的控制器类等。\n4.HandlerAdapter（处理器适配器）：\nHandlerAdapter 负责将请求分派给相应的 Controller 进行处理。 HandlerAdapter 根据不同的 Controller 类型调用相应的方法，并处理请求的参数绑定、数据转换、验证等操作。\n5.ViewResolver（视图解析器）：\nViewResolver 将逻辑视图名称解析为实际的视图对象。 Spring MVC 提供了多种 ViewResolver 的实现，包括 InternalResourceViewResolver（用于解析 JSP 视图）、ThymeleafViewResolver（用于解析 Thymeleaf 模板视图）等\n6.View（视图）：\nView 负责渲染模型数据并生成最终的响应内容。 不同的视图技术有不同的实现，如 JSP 视图、Thymeleaf 视图、FreeMarker 视图等\n。\n2.springmvc的执行流程 用户向服务器发送请求,请求被springmvc前端控制器dispatcherservlet捕获 dispatcherservlet对请求去url进行解析,得到请求次元标识符url,判断url对应的映射: 不存在: 1.再判断配置了mvc:default-servlet-handler 2.如果没有配置,则控制台包映射查找不到,客户端展示404错误 3.如果有配置:测访问目标资源(一般为静态资源),找不到客户端也会报404的错误 存在: 根据url,调用handlereMapping获得该handler配置的所有相关对象(包括handler对象以及handler对象对应的拦截器),最后以handlerExecutionChain执行链对象的形式返回 2. dispatcherservlet根据获得的handler选择一个合适的handleradapter 如果成功获得handleradapter,此时将开始执行拦截器的prehandler方法(正向) 提取request中的模型数据,填充handler入参,开始执行handler方法,处理请求,再填充handler的入参过程中,根据你的配置,spring将帮你做一些额外的工作 (1).httpmessagerconveter:将请求消息转换为一个对象,将对象转换为指定的相应消息 (2).数据转换:将请求消息进行数据转换,如string转换为integer等 (3).数据格式化:将请求消息进行数据格式化,将字符串转化为格式化数字或者格式化日期 (4).数据验证:检验数据的有效性(长度,格式),将验证结果储存到bindingresult或者error中 - handler执行完成后,向dispatcherservlet返回一个modelandview对象 此时将开始执行拦截器的posthandle方法逆向 根据返回的modelandview(此时会判断是否存在异常,如果存在异常则执行handlerresolver进行异常处理)选择一个适合的viewresolver进行视图解析,根据model和view,来渲染视图 渲染视图完毕执行拦截器的aftercompletion方法 将渲染结果返回给客户端 代码展示: 用户向服务器发送请求, 调用DispatcherServlet中继承下来的service方法,对请求方式进行判断\n1 2 3 4 5 6 7 8 9 protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { HttpMethod httpMethod = HttpMethod.resolve(request.getMethod()); if (httpMethod != HttpMethod.PATCH \u0026amp;\u0026amp; httpMethod != null) { super.service(request, response); } else { this.processRequest(request, response); } } 然后调用this.processRequest(request, response);在这个方法中紧接着调用了this.doService(request, response);再调 this.doDispatch(request, response);\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { try { ModelAndView mv = null; Exception dispatchException = null; try { processedRequest = this.checkMultipart(request); multipartRequestParsed = processedRequest != request; /*根据handlerMapping获取到执行连对象,执行链对象中有三个属性 private final Object handler;执行器对象 private List\u0026lt;HandlerInterceptor\u0026gt; interceptorList;配置的拦截器 private int interceptorIndex;用于记录拦截器的执行个数，起始为-1*/ mappedHandler = this.getHandler(processedRequest); if (mappedHandler == null) { this.noHandlerFound(processedRequest, response); return; } // 根据this.getHandlerAdapter(mappedHandler.getHandler())获取到合适的执行器适配器对象 HandlerAdapter ha = this.getHandlerAdapter(mappedHandler.getHandler()); String method = request.getMethod(); boolean isGet = \u0026#34;GET\u0026#34;.equals(method); if (isGet || \u0026#34;HEAD\u0026#34;.equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if ((new ServletWebRequest(request, response)).checkNotModified(lastModified) \u0026amp;\u0026amp; isGet) { return; } } //执行连接器的的PreHandle方法，执行顺序按照配置的先后顺序，其中默认的是第一个执行的 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } //执行handler方法实现controller中的方法，并返回一个modelAndView对象 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } this.applyDefaultViewName(processedRequest, mv); //执行拦截器中的PostHandle方法，执行顺序按照配置的反序进行 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception var20) { dispatchException = var20; } catch (Throwable var21) { dispatchException = new NestedServletException(\u0026#34;Handler dispatch failed\u0026#34;, var21); } this.processDispatchResult(processedRequest, response, mappedHandler, mv, (Exception)dispatchException); } catch (Exception var22) { //执行拦截器中的AfterCompletion(方法，执行顺序按照配置的反序进行 this.triggerAfterCompletion(processedRequest, response, mappedHandler, var22); } catch (Throwable var23) { this.triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\u0026#34;Handler processing failed\u0026#34;, var23)); } } finally { if (asyncManager.isConcurrentHandlingStarted()) { if (mappedHandler != null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else if (multipartRequestParsed) { this.cleanupMultipart(processedRequest); } } } ","date":"2023-06-04T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/springmvc%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6/","title":"springmvc的核心组件"},{"content":"什么是堆？\n堆（英语：heap)是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵树的数组对象。堆总是满足下列性质： 堆中某个节点的值总是不大于或不小于其父节点的值； 堆总是一棵完全二叉树。 将根节点最大的堆叫作最大堆,根节点最小的堆叫做最小堆\n以最大堆为例:\n堆的添加操作: 将新增元素放入数组的末尾,不断去和其父亲节点做比较,大于父亲节点则交换位置,直到不满足条件或者到根节点\n堆的删除操作: 堆的删除就是把根节点去除,然后我们可以把数组当中的最后一个元素去放入根节点中, 左右孩子的最大值做对比,如果小于左右孩子的最大值就与其进行交换,重复上述操作,知道不满足条件或者到叶子节点\n堆的代码实现:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 public class MaxHeap\u0026lt;E extends Comparable\u0026lt;E\u0026gt;\u0026gt; { //自定义的动态数组 private DynamicArray\u0026lt;E\u0026gt; data; public MaxHeap(int capacity) { data = new DynamicArray\u0026lt;\u0026gt;(capacity); } public MaxHeap(E[] arr){ data=new DynamicArray\u0026lt;\u0026gt;(arr); for (int i = parent(arr.length-1); i \u0026gt;=0; i--) { siftDown(i); } } public MaxHeap() { data = new DynamicArray\u0026lt;\u0026gt;(); } public int size() { return data.getSize(); } public boolean isEmpty() { return data.isEmpty(); } private int parent(int index) { if (index == 0) { throw new IllegalArgumentException(); } return (index - 1) / 2; } private int leftChild(int index) { return index * 2 + 1; } private int rightChild(int index) { return index * 2 + 2; } public void add(E e) { data.addLast(e); siftUp(data.getSize() - 1); } private void siftUp(int k) { while (k\u0026gt;0\u0026amp;\u0026amp;data.get(parent(k)).compareTo(data.get(k)) \u0026lt; 0) { data.swap(k, parent(k)); k=parent(k); } } public E findMax() { return data.get(0); } public E extractMax() { E remove = data.get(0); E e = data.remove(size() - 1); data.add(0, e); siftDown(0); return remove; } private void siftDown(int i) { while (leftChild(i)\u0026lt;size()) { int j = leftChild(i); if (j + 1 \u0026lt; size() \u0026amp;\u0026amp; data.get(j + 1).compareTo(data.get(j)) \u0026gt; 0) { j++; } if (data.get(i).compareTo(data.get(j)) \u0026gt;= 0) { break; } data.swap(i, j); i = j; } } public E replace(E e) { E remove = data.get(0); data.add(0, e); siftDown(0); return remove; } ","date":"2023-05-15T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A0%86/","title":"数据结构-堆"},{"content":"Spring IOC和AOP :1.Spring是轻量级的开源的JavaEE框架\n2.Spring可以解决企业应用开发的复杂性\n3.Spring有两个核心部分:IOC和Aop\nIOC:控制反转,把创建对象过程交给Spring进行管理\nAop:面向切面,不修改源代码进行功能增强\n4.Spring的特点\n​ 1.方便解耦,简化开发\n​\t2.Aop编程支持\n​\t3.方便程序测试\n​\t4.方便和其他框架进行整合\n​\t5.方便进行事务操作\n​\t6.降低API开发难度\nIOC容器: IOC是控制反转的简写\n1.控制反转,把对象创建和对象之间的调用过程,交给spring进行管理\n2.使用IOC的目的:为了降低耦合度\nIOC的底层原理\nxml解析,工厂模式,反射\n[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-0R9xeagn-1681616394864)(C:\\Users\\lenovo\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230415082118093.png)]\nIOC(接口)\n1.IOC思想基于IOC容器完成的底层就是对象工厂\n2.Spring提供IOC容器实现的两种方式:(两个接口)\n(1)BeanFactory:IOC容器基本实现,是Spring内部的使用接口,不提供开发人员进行使用\n加载配置文件时候不会创建对象,在获取对象(使用)才会去创建对象\n(2)ApplicationContext:BeanFactory的子接口,提供更多强大的功能,一般由开发人员进行使用\n加载配置文件时候就会把配置文件对象进行创建\n重要的实现类 FileSystemXmlApplicationContext使用的xml路径要求是绝对路径\nClassPathXmlApplicationContext使用的xml文件路径要求是相对路径\nIOC操作Bean管理 (1)Spring创建对象 (2)Spring 注入属性\n两种实现方式:1.通过配置文件实现\n注入方式DI(依赖注入),是IOC的一种具体\n实现 支持set方法注入和有参方法注入\n使用相应方法时一定要有相关的set方法或者构造方法,否则会报错\n1 2 3 4 \u0026lt;bean id=\u0026#34;user\u0026#34; class=\u0026#34;Test.User\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;张三\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; value=\u0026#34;18\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; 1 2 3 4 \u0026lt;bean id=\u0026#34;teacher\u0026#34; class=\u0026#34;Test.Teacher\u0026#34;\u0026gt; \u0026lt;constructor-arg index=\u0026#34;0\u0026#34;/(name=\u0026#34;\u0026#34;) value=\u0026#34;赵四\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg index=\u0026#34;1\u0026#34;/(name=\u0026#34;\u0026#34;) value=\u0026#34;26\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; 表现表示的是属性,它使用的是set方法注入并先通过无参构造来进行创建再调用set方法注入属性值,一定要用相关属性的set方法\n标签表示使用有参方法来进行直接注入属性创建对象,使用的是有参的构造方法来进行创建的,可以使用name=\u0026ldquo;\u0026ldquo;来书写相应的属性值名称,也可以使用index来调用相应构造方法参数的顺序来进行赋值,一定要有相应的构造方法\n通过p名称来进行配置\n[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-m4Ml8HVx-1681616394864)(C:\\Users\\lenovo\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230415090851925.png)]\n1 \u0026lt;bean id=\u0026#34;teacher\u0026#34; class=\u0026#34;Test.Teacher\u0026#34; p:age=\u0026#34;18\u0026#34; p:name=\u0026#34;张三\u0026#34; \u0026gt;\u0026lt;/bean\u0026gt; 注入null值\n1 2 3 \u0026lt;property name=\u0026#34;name\u0026#34; \u0026gt; \u0026lt;null\u0026gt;\u0026lt;/null\u0026gt; \u0026lt;/property\u0026gt; 注入喊特殊符合的字面量要么使用转义字符\n1 2 3 4 5 \u0026lt;property name=\u0026#34;name\u0026#34; \u0026gt; \u0026lt;value \u0026gt; \u0026lt;![CDATA[]]\u0026gt; \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 要不使用CDATA\n外部注入: 1 2 3 4 5 6 7 8 9 \u0026lt;bean id=\u0026#34;teacher\u0026#34; class=\u0026#34;Test.Teacher\u0026#34; \u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;张三\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; value=\u0026#34;26\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;student\u0026#34; ref=\u0026#34;student\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;student\u0026#34; class=\u0026#34;Test.User\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;赵四\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; value=\u0026#34;18\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; 内部注入: 1 2 3 4 5 6 7 8 9 10 \u0026lt;bean id=\u0026#34;teacher\u0026#34; class=\u0026#34;Test.Teacher\u0026#34; \u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;张三\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; value=\u0026#34;26\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;student\u0026#34; \u0026gt; \u0026lt;bean id=\u0026#34;student\u0026#34; class=\u0026#34;Test.User\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;赵四\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; value=\u0026#34;18\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; 普通bean你在配置文件中配置那种bean类型,就返回什么类型\n工厂bean 在配置文件定义的bean类型和返回的类型不一样\n工厂bean步骤:1.创建类,让这个类作为工厂bean,是实现接口FactoryBean\n​\t2.实现接口中的方法,在实现方法中定义返回的bean类型\n然后在重写的getObject方法中定义返回的bean类型\n注:使用工厂类时,使用getBean方法时,要填写的时你在getObject方法中定义的返回类型\nbean的作用域: 在Spring里面,默认情况下,bean是单实例对象\n如何设置是单实例还是多实例:在bean标签中有属性scope用于设置单实例还是多实例\nscope属性值:1.默认值是singleton 表示是单实例对象 2.第二个值 prototype 表示是多实例对象,request和session是创建完之后放到request和session的共享域中(在web项目中才会用这两个值)\nsingleton和prototype 的区别:\nscope值是singleton时,加载spring配置文件的时候就会创建单实例对象,\nscope值是prototype的时候,不是在加载spring配置文件的时候创建对象,在调用getBean方法的时候创建多实例对象\nbean的生命周期: 1.通过构造器创建bean实例(无参数构造)\n2.为bean的属性设置值和对其他的bean引用(调用set方法)\n3.把bean的实例传递给bean的后置处理器\n4.调用bean的初始化的方法(需要进行配置初始化的方法)\n5.把bean的实例传递给bean的后置处理器\n6.bean可以使用了(对象获取到了)\n7.当容器关闭时候,调用bean的销毁方法(需要进行配置销毁的方法)\nbean的后置处理器实现接口BeanPostProcessor,对你在配置文件创建的bean实例都起到作用\nbean自动装配: 根据指定的装配规则(属性名称或者属性类型),spring自动将匹配的属性值进行注入\nbean的标签的属性值autowrite 常用的两个值:\n​\tbyName 根据属性名称注入,注入bean的id值和类属性名称一样\n​\tbyType根据属性类型注入\nbean外部属性文件: 1.配置德鲁伊连接池\n2.引用外部属性文件配置德鲁伊数据库连接池\n直接创建\n[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-NcdRAhYn-1681616394865)(C:\\Users\\lenovo\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230415110819440.png)]\n引用外部文件创建\n1.在外部创建德鲁伊配置文件\n2.引入context名称空间\n注解: 注解可以写在类上面,方法上面,属性上面\n1.@component\n2.@service\n3.@controller\n4.@repository\n都可以用来创建bean实例,在注解中不写任何内容默认情况下是类名的首字符小写其他不变\n基于注解方式来进行对象创建:\n1.引入jar包\n2.开启组件扫描\n1 \u0026lt;context:component-scan base-package=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/context:component-scan\u0026gt; 如果要扫描多个包,多个包使用逗号隔开或者直接扫描包的上层目录\n1 2 3 \u0026lt;context:component-scan base-package=\u0026#34;Test\u0026#34; use-default-filters=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;context:include-filter type=\u0026#34;annotation\u0026#34; expression=\u0026#34;org.springframework.stereotype.Service\u0026#34;/\u0026gt; \u0026lt;/context:component-scan\u0026gt; 只扫描这个包中是service注解的类\n1 2 3 \u0026lt;context:component-scan base-package=\u0026#34;Test\u0026#34; \u0026gt; \u0026lt;context:exclude-filter type=\u0026#34;annotation\u0026#34; expression=\u0026#34;org.springframework.stereotype.Service\u0026#34;/\u0026gt; \u0026lt;/context:component-scan\u0026gt; 只不扫描这个包中为该注解的类\n基于注解方式实现属性注入 1.@Autowrited 根据属性类型进行自动装配\n2.@Quqlifier 根据属性名称进行注入和@Autowrited一起使用\n3.@Resource 可以根据属性类型注入,也可以根据属性名称进行注入\n4.@value注入不同类型属性\n@Resource不加内容就是根据属性的类型进行注入,加上名称就是根据名称进行注入\n完全注解开发: 1.创建配置类使用注解@Configuration @ComponentScan(basePackages={\u0026rdquo; 包名\u0026rdquo;})\n2.加载配置文件,使用AnnotationConfigApplicationContext(配置类.class)\nAOP: 概念:面向切面编程(方面)利用aop可以对业务逻辑的各个部分进行隔离,从而使得业务逻辑各部分之间的耦合度降低,提高程序的的可重用性,同时提高开发的效率\n(不通过修改源码方式,在主干功能里面添加功能)\n底层原理:\n1.有接口的使用jdk动态代理\n使用类Proxy 调用 newProxyInstance方法生成代理对象,该方法有3个参数\n第一个参数是,类加载器\n第二个参数是增强方法所在的类,这个实现的接口,支持多个接口\n第三个参数是,实现IncocationHandler创建代理对象写增强的那一部分\n术语: 1.连接点 :类中哪些方法可以被增强,这些方法称为连接点\n2.切入点:实际被增强的方法,称为切入点\n3.通知(增强) 实际增强的逻辑部分被称为通知(增强)\n​ 1.前置通知\n​\t2后置通知\n​\t3.环绕通知\n​\t4.异常通知\n​\t5.最终通知\n4.切面:把通知应用到切入点的过程\n切入点表达式:\n​\texecution(权限修饰符.返回类型,类全路径 ,方法名称(参数列表) )\n权限修饰符可以省略\n返回值类型省略表示任意返回值类型都可以\n参数列表可以使用 .. 表示有无参数均可，有参数可以是任意类型\n参数列表可以使用 * , 表示可以是任何的数据类型,但必须有参数\nAspect\n1.创建类,在类中建立方法\n2.创建一个增强类在增强对象上写@Aspectj 在增强类中建立不同的方法,让不同的方法代表不同通知类型\n3.进行通知配置,开启注解扫描\n相同的切入点提取\n@pointcut(value= )\npublic void pointdemo(){\n}\n之后可以直接在注解中调用这个方法\n有多个增强类同一个方法进行增强,设置增强类优先级\n@order(数字) 数字类型越小优先级越高\n1 @EnableAspectJAutoProxy(proxyTargetClass = true) 注解开启代理\n","date":"2023-04-16T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/spring-ioc%E5%92%8Caop/","title":"Spring IOC和AOP"},{"content":"Map集合的遍历方式,以及TreeSet集合的排列方式 Map集合的遍历方式: 1.键找值:\n1 2 3 4 5 Map\u0026lt;String,Integer\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); Set\u0026lt;String\u0026gt; set=map.keySet() for(String s:set){ Integer value=map.get(s); } 先创建一个Map集合然后通过Map集合中的keySet方法获得一个Set集合,集合中储存的是Map集合中的键,遍历这个Set集合获得其中的键,然后通过get()方法通过键找到相应的值\n2.键值对\n1 2 3 4 5 6 Map\u0026lt;String,Integer\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); Set\u0026lt;Map.Entry\u0026lt;String,Integer\u0026gt;\u0026gt; set=map.entrySet(); for(Map.Entry\u0026lt;String,Integer\u0026gt; s:set){ string key=s.getKey(); Integer value=s.getValue(); } 先创建一个Map集合然后通过Map集合中的entrySet方法获得一个Set集合,集合中储存的是Map集合中的键值对,遍历这个Set集合获得其中的键值对,然后通过getKey()方法找到相应的键,getValue()方法找到相应的值\n3.lambda表达式来进行遍历\n1 2 3 4 5 6 Map\u0026lt;String,Integer\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.forEach(new BiConsumer\u0026lt;String,Integer\u0026gt;){ @override public void accept(String key,Integer value){ System.out.println(key+\u0026#34;=\u0026#34;value); }}); 通过匿名内部类来进行实现 简写为\n1 2 3 4 Map\u0026lt;String,Integer\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.forEach(String key,Integer value)-\u0026gt;{ System.out.printlin(key+\u0026#34;=\u0026#34;value) }); TreeSet集合的排序方式 TreeSet集合默认的排序方式是 1.对于数据类型来说:Integer,Double等都是从小到大来进行排序的 2.对于字符和字符串类型的数据,是根据Ascll码表的数字进行升序排序的\nTreeSet集合的两种比较方式 1.默认排序或自然排序: 如果我们添加的自定义对象那么,TreeSet集合就知道如何去比较,我们就需要自己去定义排序规则,来进行排序,否则自定义对象添加到集合中运行后系统会报错 我们可以让javabean类来实现comparable接口指定比较规则 (comparable接口带有泛型) 重写compareTo方法定义规则 比如我们如果建立了一个学生类想要根据学生的年纪来进行排序\n1 2 3 4 @override public int compareTo(Student o){ return this.getAge()-o.getAge(); } this表示的是我们要添加的元素,o表示的是已经存在的元素 返回值是负数表示我们添加的数跟已经存在的数相比是小的,应该存在左边 返回值是0表示我们添加的数据跟已经存在的数据相比是相等的,认为该元素已经存在,则舍弃 返回值是正数表示的是我们添加的数据跟已经存在的数据相比是大的,应该存在右边 如果这个按年龄来排的化,年龄相等就会舍弃我们应该在前面加上有个判断,如果为零,用其他特征来进行比较\n2.比较器来进行比较: 如果我们集合中添加的不是自定义对象,那么我们想要来进行指定的规则来进行排序而不是按照默认的方式进行升序排序, 我们就无法使用第一种方法此时可以使用 创建TreeSet集合的时候来传递一个比较器Comparator来通过重写其中的comparate指定规则 例子:如果我们添加的是字符串,我们希望字符串来按照长度排序,如果长度相等按照首字母排序\n1 2 3 4 5 6 7 TreeSet\u0026lt;String,String\u0026gt; set=new TreeSet\u0026lt;\u0026gt;(new Comparator\u0026lt;String\u0026gt;(){ @override public int compare(String s1,String s2){ int i=s1.length()-s2.length(); i=i==?s1.compareTo(s2):i return i; }}); 本周在写程序的时候发现之前学的知识点很乱,代码实现其中的功能时动起手来也很难,希望在接下来的时间抽出时间来进行复习,梳理之前的知识,打好基础\n","date":"2023-03-27T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/map%E9%9B%86%E5%90%88%E7%9A%84%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F%E4%BB%A5%E5%8F%8Atreeset%E9%9B%86%E5%90%88%E7%9A%84%E6%8E%92%E5%88%97%E6%96%B9%E5%BC%8F/","title":"Map集合的遍历方式,以及TreeSet集合的排列方式"},{"content":"@TOC\nIO流 IO流用于读写文件中的数据,I:input O:output\n输出流是程序流向文件,输入流是文本流向\n他们都是抽象类,需要使用他们的实现类\nFilefOutputstream FilefInputstream\n他们在创建对象时会出现编译时异常,一定要进行异常声明\n字节流 FilefOutputstream FilefOutputstream创建的时候需要抛出异常,异常是检验输进去的路径是否存在 创建对象时参数可以是字符串也可以是file对象,如果文件不存在会创建一个新的文件但要保证它的父级路径是存在的,如果父级路径不存在就会报错\n如果文件已经存在,文件内容就会被清空\nwrite方法可以添加数据,参数是int类型的表示的是ascll码值\n释放资源防止文件被占用\n1 2 3 void write(int b) 一次写一个字节数据 void write( byte[] b) 一次写一个字节数组数据 void write(byte[], int off,int len) 一次写一个字节数组的部分数据 注:write()方法中len参数代表的是长度而不是终止索引\n可以通过把想添加的内容用字符串表示,然后通过Arrays中的getByte()方法把字符串中的每个字符转化为ascll码值并返回一个byte数组,直接用write方法调用数组直接进行添加\n换行符\\r\\n(不能直接输入,还是要用ascll码值间接)\n如果要续写需要再创建对象的时候,写第二个参数true\nFilefInputstream FileInputSream在创建对象时参数是文件的路径,如果路径不存在直接报错\n读取数据的方法read(),一次是读一个字节的,读出来的是数据在ascll上对应的数字\n读到文件末尾了,read发()方法返回-1.空格也是数据\nread()方法一次只能读一个字节数据可以使用循环来读取文件的全部数据\nread方法 1 2 public int read() 一次读一个字节数据 public int read(byte[] buffer) 一次读一个字节数组数据 read(byte[] buffer) 方法的返回值int表示本次读取到了多少个数据\n具体读多少个字节数据跟数组的长度有关\nread(byte[] buffer) 在使用时如果数组的的长度小于文件数据的长度时进行了多次读取,那么最后一次读取数组的长度大于剩余数据长度,除了读取到剩余的数据意外数组中还含有上次1读取的数据,原因是数组读取数据后,下次读取的数据会对上次读取的数据进行覆盖,但最后文件剩余数据小于数组长度只能覆盖住一部分,打印该数组时会打印出未覆盖的元素\n此时我们可以通过string(byte[],off,len)来指定打印长度\n打印文件数据 1 2 3 4 5 6 FileInputStream fis=new FileInputStream (\u0026#34;一个路径\u0026#34;); int b; while((b=fis.read())!=-1){ System.out.println((char)b) } fis.close; 这里要定义一个变量b,通过打印b来间接打印文件数据\n1 2 3 4 5 6 FileIntputStream fis=new FileInputStream (\u0026#34;一个路径\u0026#34;); int b; while((b=fis.read())!=-1){ System.out.println(fis.read();) } fis.close; 如果在打印函数中同样书写read()方法来进行打印,那么打印函数中的read方法会和while方法判断语句中的read方法一前一后轮流接着读取文件中的数据.参考迭代器中的next方法\n文件的拷贝 代码如下:\n1 2 3 4 5 6 7 8 FileInputStream fis= new FileIntputStream(\u0026#34;需要拷贝文件的路径\u0026#34;); FilefOutputStream fos =new FilefOutputstream (\u0026#34;拷贝到新文件的路径\u0026#34;); int b; while((b=fis.read())!=-1){ fos.write(b) } fos.close(); fis.close(); 1 2 3 4 5 6 7 8 9 FileInputStream fis= new FileIntputStream(\u0026#34;需要拷贝文件的路径\u0026#34;); FilefOutputStream fos =new FilefOutputstream (\u0026#34;拷贝到新文件的路径\u0026#34;); int len; byte[] bytes=new byte[1024*1024*5] while((len=fis.read(bytes))!=-1){ fos.write(bytes,0,len) } fos.close(); fis.close(); 下面的代码上面的更快\ntry catch异常 1 2 3 4 5 6 7 try{ FilefOutputStream fos =new FilefOutputstream (\u0026#34;文件的路径\u0026#34;); fos.write(97); fos.close(); }catch(IOException e){ e.printStackTrace(); } 如果我们这样写那么fos.close()前面出现异常时,会直接创建异常对象并跳到catch语句中,就无法释放资源,这时候我们可以用\nUTB-8中中文是用3个字节储存的且首位为1,英文是用1个字节储存的首位为0\nJAVA中编码的方法\n1 2 public byte[] getBytes() 使用默认方式进行编码 public byte[] getBytes(String charsetName) 使用指定方式进行编码 JAVA中解码的方法\n1 2 String (byte[] bytes) 使用默认方式进行解码 String (byte[] bytes, String charsetName ) 使用指定方式进行解码 字符流 FileReader 构造方法 1 2 public FileReader(File file) 创建字符输入流关联本地文件 public Filereader(String pathname) 创建字符输入流关联本地文件 读取数据方法 1 2 public int read() 读取单个数据,读到末尾返回-1 public int read(char[] buffer) 读取多个数据,读到末尾返回-1 按字节进行读取,遇见中文,一次读多个字节,读取后解码,返回一个整数(方法的底层会对所读取的字节二进制数进行解码并转化为十进制数,返回十进制数,这些十进制数也是字符集上的数,想看到这些内容就要进行强转)\n读到文件末尾了,read方法返回-1\nread的有参方法中包含了强制转换这一步,并将转换后的字符放到字符数组中\nFileWriter 构造方法 1 2 3 4 public FileWrite(File file) 创建字符输出流关联本地文件 public FileWrite(String pathname) 创建字符输出流关联本地文件 public FileWrite(File file, boolean append) 创建字符输出流关联本地文件,续写 public FileWrite(String pathname,boolean append) 创建字符输出流关联本地文件,续写 写出数据方法 1 2 3 4 5 void write(int c) 写一个字符 void write(String str) 写一个字符串 void write(String str,int off,int len) 写一个字符串的一部分 void write(char[] cbuf) 写出一个字符数组 void write(char[] cbuf,int off,int len) 写出字符数组的一部分 缓冲流 字节缓冲流: 1 2 public BufferedInputStream( InputStream is) 将基本流包装为高级流提高读出数据性能 public BufferedOutputStream( OutputStream os ) 将基本流包装为高级流提高写出数据性能 原理:底层自带了胀肚为8192的缓冲区提高性能\n释放资源时不需要对字节流进行关闭,只需要关闭字节缓冲流,字节缓冲流的底层代码中有关闭字节流的代码\n字符缓冲流 构造方法和字节缓冲流类似\n特有的方法:\n​\n1 2 public String readLine() 读取一行的数据,如果没有数据课读了就返回null public void newLine() 跨平台的换行 new方法会判断你是什么平台然后弄相关的换行符\n缓冲流为什么能提高性能:\n1.缓冲流自带8192的字节数组缓冲区\n2.利用显著提高字节流的读写性能\n3.对于字节流提升并不明显,对于字符缓冲流而言关键点是两个特有的方法\n(字符流中也含有缓冲区)\n转换流 将字节流转换为字符流\n转换流是一种处理流，它可以将一种数据类型转换为另一种数据类型。\n它通常用于解决两个系统之间数据格式不兼容的问题。例如，在将字符串从一个系统传输到另一个系统时，转换流可以将字符串转换为二进制数据，并在接收系统中将其转换回字符串。\n转换流也可以用于将不同编码的字符串转换为另一种编码的字符串，或者将一种数据类型转换为另一种数据类型以便在程序中使用。\n总的来说，转换流允许我们在不同类型的数据之间转换和传输数据，使得不同的系统和应用程序能够进行交互。\n转换输入流InputStreamReader 转换输出流OutputStreamWriter\n字节流使用字符流中的方法\n1 2 InputStreamReader Isr=new InputStreamReader(Fileinputstream file,String charsname) OutputStreamWriter Isr=new OutputStreamWriter(FileOutputstream file,String charsname) 打印流 打印流分为两类:printstream printwrite\n打印流只能操作文件的目的地,不操作数据源\n特有的方法可以实现,数据原样写出,特有的写出方,可以实现自动刷新自动换行\n字节打印流 构造方法 1 2 3 4 public PrintStream(OutputStream /File/String) 关联字节输出流/文件/文件路径 public PrintStream( String filename,Charset charset ) 指定字符编码 public PrintStream(OutputStream,boolean autoflush) 自动刷新 public PrintStream(OutputStream,boolean autoflush,String encoding) 指定字符编码且自动刷新 字节流是没有缓冲区的,开不开自动刷新都一样\n成员方法 1 2 3 4 public void write(int b) 将指定的字节打印出来 public void println(Xxx xx) 打印任意数据,自动刷新,自动换行 public void print(Xxx xx) 打印任意数据,不换行 public void printf(String format,Object....args ) 带有占位符的打印语句,不换行 后三个方法可以实现数据的原样写出,是打印流特有的方法\n字符打印流 构造方法和成员变量与字节打印流基本相同\n序列流 序列流可以把Java中的对象写道本地文件中\n序列化流的构造方法和成员变量\n1 2 public objectOutputStream(OutputStream out) 把基本流包装为高级流 public final void writeObject(Object obj) 把对象序列化写到文件中 使用对象输出流保存到文件时会出现NotSerializableException异常\n解决方案:需要让javabean类实现Serializable接口,表明这个javabean类可以被序列化,这个接口没有内容是一个标记接口\n反序列化的构造方法和成员变量\n1 2 public objecyInputStream(InputStream out) 把基本流包装为高级流 public final void readObject(Object obj) 把序列化到文件的对象,读取到程序中 序列化流中如果教案javabean类内容发生改变,反序列化流就无法读取序列化到文件的对象,每一个javabean类都有一个版本号,我们可以通过声明一个固定的版本号防止版本号发生变化private static final long serialVersionUID=版本号l\n","date":"2023-03-03T14:09:41+08:00","permalink":"https://zzjjjjjjx/zjx.github.io/p/io%E6%B5%81/","title":"IO流"}]